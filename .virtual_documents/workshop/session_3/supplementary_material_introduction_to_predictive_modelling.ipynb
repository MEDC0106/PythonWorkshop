


from sklearn import datasets  # import the datasets module

# Download the dataset:
dataset = datasets.load_breast_cancer(as_frame=True)





# Print the targets in the dataset
print('Targets:', dataset.target_names)


# Print the names of the features in the dataset
print('Features:', dataset.feature_names)





target = dataset.target  # Our class labels
target.unique()


features = dataset.data  # Our features
features.head(5)





target.value_counts()  # remember this from the last notebook?





target = 1 - target  # simple way to flip the labels!





# We add this 'Jupyter magic' to display plots in the notebook.
%matplotlib inline
import matplotlib.pyplot as plt

# Construct the plot:
target.value_counts(sort=True).plot.bar()

# Set the axis labels:
plt.xlabel('Target classification')
plt.ylabel('Count')
plt.xticks([0, 1], ['benign', 'malignant']);





features.info()


features.describe()





# Create a histogram:
features['mean radius'].plot.hist();


# Create a density plot:
features['mean texture'].plot.density();





import seaborn as sns  # import seaborn

# we will restrict the columns to ones that contain the 'mean' features
cols =  [x for x in features.columns if 'mean' in x]

# Now we can construct the plot (may take a few seconds)
sns.pairplot(dataset.frame, hue='target', vars=cols);





# First calculate the correlation matrix
correlation = features.corr()

# Plot the result (here we set up a matplotlib figure to specify the size)
plt.figure(figsize=(16,12)) 
sns.heatmap(correlation);





from scipy.stats import pearsonr

col = 'mean concave points'
result = pearsonr(features[col], target)

print('Correlation:', result[0])





import pandas as pd

# We can construct a dataframe to store our correlations:
rankings = pd.DataFrame({
    'feature': features.columns
})

# Calculate the pearson r-squared for each feature:
rankings['pearsonr'] = rankings.feature.apply(lambda x: pearsonr(features[x], target)[0])

# lets sort the features by pearsonr and display most negatively correlated:
rankings.sort_values('pearsonr', inplace=True)
rankings.reset_index(inplace=True, drop=True)

# Plot the result
ax = rankings['pearsonr'].plot.barh(figsize=(12, 10))
ax.set_title('Feature vs Target Pearson Correlation')
ax.set_yticklabels(rankings.feature)
ax.set_xlabel('Correlation');





from sklearn.preprocessing import StandardScaler

# First setup the scaler:
scaler = StandardScaler() 

# Now scale the features (now a NumPy Array):
scaled_X = scaler.fit_transform(features)





from sklearn.decomposition import PCA

# We will reduce the dimensions to 2:
pca = PCA(n_components=2)

# Transform the data
Xt = pca.fit_transform(scaled_X)

# Check the output shape!
print('New Shape:', Xt.shape)

# We can save these components to a new DataFrame
components = pd.DataFrame(Xt, columns=['PC1', 'PC2'])

# Add the targets to help with plotting
components['Target'] = target
components.head(3)





# Convert binary labels to text:
components['Target'] = components.Target.apply(lambda x: 'Benign' if x == 0 else 'Malignant')

# Create scatter plot:
plt.figure(figsize=(10, 8)) # Make figure larger!
sns.scatterplot(x='PC1', y='PC2', hue='Target', data=components);





import numpy as np

pca = PCA().fit(scaled_X)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance');








from sklearn.model_selection import train_test_split

# Split the features and labels (NumPy arrays)
x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)

print('X train shape:', x_train.shape)
print('X test shape:', x_test.shape)
print('Y train shape:', y_train.shape)
print('Y test shape:', y_test.shape)





from sklearn.linear_model import LogisticRegression

# Setup our standard scaler
scaler = StandardScaler()

# Scale our training features
x_train_scaled = scaler.fit_transform(x_train)

# Initialize our model (default parameters)
lr = LogisticRegression()

# Fit our model on the scaled training data (features, target)
lr.fit(x_train_scaled, y_train)








# Scale our tetsing features
x_test_scaled = scaler.transform(x_test)

# Make our predictions (binary)
y_pred = lr.predict(x_test_scaled)
print('Predictions:', y_pred)





# calculate and round to three d.p.
accuracy_score = round(lr.score(x_test_scaled, y_test), 3)

print('Accuracy of logistic regression model on test data:', str(accuracy_score * 100) + '%')





from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix

# Construct a confusion matrix (NumPy array)
c_m = confusion_matrix(y_true=y_test, y_pred=y_pred)

# Plot the confusion matrix
ConfusionMatrixDisplay(c_m, display_labels=['Benign', 'Malignant']).plot(cmap='Blues');





from sklearn.metrics import RocCurveDisplay, roc_curve, auc

# First calculate our FPR and TPR
fpr, tpr, thresholds = roc_curve(y_test, y_pred)

# Calculate the AUC from FPR and TPR
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='Logistic Regression').plot();








coefficients = lr.coef_[0]

feat_importance = pd.DataFrame({
    'feature': features.columns,
    'coef': coefficients
})

feat_importance.sort_values('coef', inplace=True)
ax = feat_importance['coef'].plot.barh(figsize=(12, 10))
ax.set_title('Logistic Regression Coefficients')
ax.set_yticklabels(feat_importance.feature)
ax.set_xlabel('Correlation');









