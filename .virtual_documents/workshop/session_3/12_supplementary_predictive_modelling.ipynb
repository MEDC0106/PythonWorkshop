


from sklearn import datasets  # Import the `datasets` module

dataset = datasets.load_breast_cancer(as_frame=True)

# No output is expected from this cell





# Print the targets in the dataset
print('Targets:', dataset.target_names)


# Print the names of the features in the dataset
print('Features:', dataset.feature_names)





target = dataset.target  # Our class labels
target.unique()


features = dataset.data  # Our features
features





target.value_counts()





target = 1 - target  # Simple way to flip the labels
target.value_counts()





# We add this 'Jupyter magic' to display plots in the notebook
%matplotlib inline
import matplotlib.pyplot as plt

# Construct the plot
target.value_counts(sort=True).plot.bar()

# Set the axis labels and x-axis ticks
plt.xlabel('Target classification')
plt.ylabel('Count')
plt.xticks([0, 1], ['benign', 'malignant'], rotation=0);





features.info()


features.describe()





# Create a histogram
features['mean radius'].plot.hist();


# Create a density plot
features['mean texture'].plot.density();





import seaborn as sns

# We will restrict the columns to ones that contain the 'mean' features
cols = [x for x in features.columns if 'mean' in x]

# Now we can construct the plot
sns.pairplot(dataset.frame, hue='target', vars=cols);

# You may see a `FutureWarning` error message
# The plots may take a while to load





# First calculate the correlation matrix
correlation = features.corr()

# Plot the result 
# Here we set up a Matplotlib figure to specify the size
plt.figure(figsize=(16,12)) 
sns.heatmap(correlation);





from scipy.stats import pearsonr

result = pearsonr(features['mean concave points'], target)
print('Correlation:', result[0])





import pandas as pd

# Step 1: Construct a DataFrame to store feature names and their correlation values
rankings = pd.DataFrame({
    'feature': features.columns  # The 'feature' column holds the feature names
})

# Step 2: Calculate the Pearson correlation coefficient for each feature
# We use an anonymous function to compute Pearson r between each feature and the target
rankings['pearsonr'] = rankings.feature.apply(lambda x: pearsonr(features[x], target)[0])

# Step 3: Sort the features by Pearson r value, placing the most negatively correlated at the top
rankings.sort_values('pearsonr', inplace=True)
rankings.reset_index(inplace=True, drop=True)

# Step 4: Plot the correlation results as a horizontal bar chart
ax = rankings['pearsonr'].plot.barh(figsize=(12, 10))
ax.set_title('Feature vs. Target Pearson correlation');
ax.set_yticklabels(rankings.feature);  # Set y-axis labels as feature names
ax.set_xlabel('Correlation');  # Label the x-axis as 'Correlation'
ax.set_ylabel('Feature'); # Label the y-axis as 'Feature'





from sklearn.preprocessing import StandardScaler

# Step 1: Setup the scaler
scaler = StandardScaler() 

# Step 2: Fit the scaler to the feature data and transform it
# This fits the scaler to the features dataset, calculating the mean and standard deviation for each feature,
# and then transforms each feature by subtracting the mean and dividing by the standard deviation
scaled_X = scaler.fit_transform(features)  # This results in a scaled dataset where each feature has a mean of 0 and a standard deviation of 1

# No output is expected from this cell





from sklearn.decomposition import PCA

# Step 1: Create an instance of PCA to reduce dimensionality
# Setting n_components=2 will reduce the dataset to two principal components, making it easier to visualise
pca = PCA(n_components=2)

# Step 2: Fit the PCA model to the scaled data and transform it
# This step calculates the two principal components that capture the maximum variance in the data 
# and then transforms the original data to this reduced two-dimensional space
Xt = pca.fit_transform(scaled_X)

# Display the shape of the transformed data to verify the reduction
print('New shape:', Xt.shape)

# Step 3: Create a new DataFrame to store the principal components
# This DataFrame includes two columns: PC1 and PC2, representing the first and second principal components
components = pd.DataFrame(Xt, columns=['PC1', 'PC2'])

# Step 4: Add the target labels for easier visualisation of the classes
components['Target'] = target
components





# Convert binary labels to text
components['Target'] = components.Target.apply(lambda x: 'Benign' if x == 0 else 'Malignant')

# Create scatter plot
plt.figure(figsize=(10, 8))
sns.scatterplot(x='PC1', y='PC2', hue='Target', data=components);





import numpy as np

pca = PCA().fit(scaled_X)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance');





from sklearn.model_selection import train_test_split

# Split the features and labels (NumPy arrays)
x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)

print('X train shape:', x_train.shape)
print('X test shape:', x_test.shape)
print('Y train shape:', y_train.shape)
print('Y test shape:', y_test.shape)





from sklearn.linear_model import LogisticRegression

# Setup our standard scaler
scaler = StandardScaler()

# Scale our training features
x_train_scaled = scaler.fit_transform(x_train)

# Initialize our model (default parameters)
lr = LogisticRegression()

# Fit our model on the scaled training data (features, target)
lr.fit(x_train_scaled, y_train)








# Scale our testing features
x_test_scaled = scaler.transform(x_test)

# Make our predictions
y_pred = lr.predict(x_test_scaled)
print('Predictions:', y_pred)





# Calculate and round to three decimal places
accuracy_score = round(lr.score(x_test_scaled, y_test), 3)

print('Accuracy of Logistic Regression model on test data:', str(accuracy_score * 100) + '%')





from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix

# Construct a confusion matrix (NumPy array)
c_m = confusion_matrix(y_true=y_test, y_pred=y_pred)

# Plot the confusion matrix
ConfusionMatrixDisplay(c_m, display_labels=['Benign', 'Malignant']).plot(cmap='Blues');





from sklearn.metrics import RocCurveDisplay, roc_curve, auc

# First calculate our FPR and TPR
fpr, tpr, thresholds = roc_curve(y_test, y_pred)

# Calculate the AUC from FPR and TPR
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='Logistic Regression').plot();





# Extract coefficients from the trained Logistic Regression model
coefficients = lr.coef_[0]

# Create a DataFrame to store feature names and their corresponding coefficients
feat_importance = pd.DataFrame({
    'feature': features.columns,
    'coef': coefficients
})

# Sort features by their coefficient values
feat_importance.sort_values('coef', inplace=True)

# Plot the feature importance as a horizontal bar chart
ax = feat_importance['coef'].plot.barh(figsize=(12, 10))
ax.set_title('Logistic Regression coefficients');
ax.set_yticklabels(feat_importance.feature);
ax.set_xlabel('Correlation');






