{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEDC0106: Bioinformatics in Applied Biomedical Science\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../../resources/static/Banner.png\" alt=\"MEDC0106 Banner\" width=\"90%\"/>\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "---------------------------------------------------------------\n",
    "\n",
    "# 07 - Introduction to Predictive Modelling\n",
    "\n",
    "*Written by:* Oliver Scott\n",
    "\n",
    "**This notebook provides a general introduction to predictive modelling using scikit-learn.**\n",
    "\n",
    "Do not be afraid to make changes to the code cells to explore how things work!\n",
    "\n",
    "-----\n",
    "\n",
    "### What is Predictive Modelling?\n",
    "\n",
    "In short, predictive modelling is the application of statistical models to predict future outcomes. Typically a predictive model includes some form of machine learning algorithm, which is trained using existing observations in order to make a prediction given new observations. Predictive modelling can be generalised into two main classes; regression and classification.\n",
    "\n",
    "**Regression:**\n",
    "\n",
    "Regression models are based on the analysis of relationships between a dependent variable ('outcome'/'response' variable) and one or more independent variables ('predictors'/'features'), e.g. predicting the output of a biological system under different conditions. \n",
    "\n",
    "**Classification:**\n",
    "\n",
    "Classification models aim to assign discrete class labels to a set of independent variables rather than a continuous value as in regression. For example a classification model could be used to diagnose a condition (or not) based on measurements such as gene expression.\n",
    "\n",
    "To keep it simple, in this notebook we will focus on a supervised classification objective, predicting predefined class labels for a set of observations.\n",
    "\n",
    "### Supervised?\n",
    "\n",
    "Classification tasks can be further grouped into two main categories: supervised an unsupervised. In supervised learning, we know the the class labels for training data *a priori* hence we can use this knowledge to 'supervise' the models learning process. the following image illustrates a classification task for samples with two random variables (x1, x2), where the class is indicated by the colour and the dotted line represents the (linear) decision boundary used to define two decision regions. New observations will be assigned to a respective class depending on in which decision region they will fall into. We can make the assumption that our model given unseen observations will not be be completely accurate miss-classifying some percentage of input samples.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://scipython.com/static/media/uploads/blog/logistic_regression/decision-boundary.png\" alt=\"Desicion boundry\" width=\"30%\"/>\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "In contrast to supervised classification, unsupervised approaches are used when the labels for a set of observations are not known *a priori* and must be inferred from the observations themselves. Typically an unsupervised algorithm consists of some form of 'clustering' algorithm, grouping data into clusters (groups) based on some form of distance (or similarity) measurement. In this notebook we will run through a basic pipeline for constructing a supervised classification model.\n",
    "\n",
    "### What is scikit-learn?\n",
    "\n",
    "[scikit-learn](https://scikit-learn.org/stable/) (sklearn) is the most popular and robust Python package for machine learning. It provides a selection of efficient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction. The full set of features can be seen at the [projects website](https://scikit-learn.org/stable/). In this notebook we will be utilising scikit-learn to construct a supervised classification model.\n",
    "\n",
    "-----\n",
    "\n",
    "# Contents\n",
    "\n",
    "1. [Objective](#Objective)\n",
    "2. [Data Analysis](#Data-Analysis)\n",
    "3. [Dimensionality Reduction](#Dimesnionality-Reduction)\n",
    "4. [Training Models](#Training-Models)\n",
    "5. [Model Evaluation](#Model-Evaluation)\n",
    "6. [Feature Importance](#Feature-Importance)\n",
    "7. [Discussion](#Discussion)\n",
    "\n",
    "-----\n",
    "\n",
    "#### Extra Resources:\n",
    "\n",
    "- [scikit-learn documentation](https://scikit-learn.org/stable/)\n",
    "\n",
    "-----\n",
    "\n",
    "#### References:\n",
    "\n",
    "- [scikit-learn documentation](https://scikit-learn.org/stable/)\n",
    "\n",
    "-----\n",
    "\n",
    "## Objective\n",
    "\n",
    "As mentioned previously, in this notebook we will be learning how to construct a supervised classification model. Specifically we will be using the 'Breast Cancer Wisconsin (Diagnostic) dataset' created by  Dr. William H. Wolberg, physician at the University Of Wisconsin Hospital at Madison, Wisconsin, USA. The features are \n",
    "computed from an image of a fine needdle aspirate (FNA) of a breast mass. A computer software 'Xcyt' was used to describe characteristics of the cell neuclei present in the image. The program constructs 10 features using a curve-fitting algorithm and then calculates the mean value, extreme value and standard error for each feature, resulting in 30 real-valued features. The process is described in [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://jithinjk.github.io/blog/images/histo/pcam.png\" alt=\"breastcancer\" width=\"100%\"/>\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "[image source](https://jithinjk.github.io/blog/images/histo/pcam.png)\n",
    "\n",
    "**Attributes:**\n",
    "\n",
    "Target:\n",
    "\n",
    "- Malignant or Benign\n",
    "\n",
    "Features:\n",
    "\n",
    "- Radius (mean of distances from center to points on the perimeter)\n",
    "- Texture (standard deviation of gray-scale values)\n",
    "- Perimeter\n",
    "- Area\n",
    "- Smoothness (local variation in radius lengths)\n",
    "- Compactness (perimeter^2 / area - 1.0)\n",
    "- Concavity (severity of concave portions of the contour)\n",
    "- Concave points (number of concave portions of the contour)\n",
    "- Symmetry\n",
    "- Fractal dimension (“coastline approximation” - 1)\n",
    "\n",
    "The mean, standard error and “worst” or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n",
    "\n",
    "Class distribution: 357 benign, 212 malignant\n",
    "\n",
    "**Objective**\n",
    "\n",
    "Our objective is to construct a model capable of classifying wether the breast sample is either malignant or benign using the given features and a supervised classification algortithm.\n",
    "\n",
    "**Talking Points:**\n",
    "\n",
    "- Do you think that using a machine learning model that can diagnose cancer accurately would be beneficial in a clinical setting?\n",
    "- Can you think of any reasons why a machine learning model might not be used to make such diagnoses?\n",
    "- Can you identify any other biomedical related (or not) problems that may benefit from predictive modelling?\n",
    "\n",
    "-----\n",
    "\n",
    "## Data Analysis\n",
    "\n",
    "Before we even think about building a model we need to understand the data we have been given/collected. Probably the most important step is actually getting the data into Python. Luckily scikit-learn provides a method for retrieving this paticular dataset. scikit-learn is imported with the name sklearn (hyphens `-` are not allowed in Python package/module names):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets  # import the datasets module\n",
    "\n",
    "# Download the dataset:\n",
    "dataset = datasets.load_breast_cancer(as_frame=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset object contains some useful attributes to get some intial information about the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the targets in the dataset\n",
    "print('Targets:', dataset.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the names of the features in the dataset\n",
    "print('Features:', dataset.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn dataset also contains the actual data as a Pandas `DataFrame` (features) and a Pandas `Series` (target). Let's take a look at these and assign them to variables to use later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = dataset.target  # Our class labels\n",
    "target.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = dataset.data  # Our features\n",
    "features.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the targets are binary encoded [0, 1] lets check how many there are of each label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.value_counts()  # remember this from the last notebook?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there are 212 examples of class 0 'malignant' and 357 examples of class 1 'benign'. I would prefer that the class labels were the other way around, 0 being 'benign' and 1 being 'malignant' so that it reflects a real-life scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 1 - target  # simple way to flip the labels!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the class counts to get a nice visual representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add this 'Jupyter magic' to display plots in the notebook.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Construct the plot:\n",
    "target.value_counts(sort=True).plot.bar()\n",
    "\n",
    "# Set the axis labels:\n",
    "plt.xlabel('Target classification')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['benign', 'malignant']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets take a look at the features, remember that we can use `.info()` and `.describe()` to get some nice global summaries. Check for any null values and make sure the data types make sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the dataset is nice and clean, no null values! All of the features are also real-valued, saving us from some extra work. When we have discrete features we need to use a method to 'encode' these features into numerical values. A technique called [one-hot-encoding](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/) is often used in this case.\n",
    "\n",
    "Looking at raw numbers can often be hard to interpret so visualising the distributions of features can be insightful. We can use pandas to plot these distributions using a histogram or a [density plot](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.density.html). Let's try visualise a couple feature distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram:\n",
    "features['mean radius'].plot.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a density plot:\n",
    "features['mean texture'].plot.density();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What observations can you make about the features? Try plotting some further features to make further observations.\n",
    "\n",
    "From the distributions of our data we can see that each feature occupies a different value range, although all are approximately [normally distributed](https://en.wikipedia.org/wiki/Normal_distribution). This observation tells us that we may benefit from scaling the features so that they occupy the same value range.  \n",
    "\n",
    "**Feature scaling** is an important part of data-preprocessing, as some machine learning alogithms will not function correctly without it. For example some classifiers use a distance measurement between two points. If one feature has a large range of values then this distance measurment will be influenced greatly by this feature opposed to a feature with a smaller range. Feature scaling brings all data into the same range so that each feature contributes approximatley  proportionately to the distance. Note that not all machine learning algorithms require that input data is scaled.\n",
    "\n",
    "In some cases you may want to **normalise** features so that features with strange distributions become more 'normal'. Normally distributed features tend to lead to better models because there is an approximately equal number of observations above and below the mean. Many models maker the assumption that your data is normally distributed. \n",
    "\n",
    "The difference between scaling and normalisation is that scaling changes the **range** of your data whereas normalisation changes the **shape** of the distribution. Therefore normalisation is useful when you know your data is not normally distributed and scaling is useful when your data is normally distributed but occupies different value ranges. There is a nice guide [here](https://towardsai.net/p/data-science/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff) if you need further explanation.\n",
    "\n",
    "Always look at your data!\n",
    "\n",
    "-----\n",
    "\n",
    "**Looking for correlations**\n",
    "\n",
    "Before we perform any feature scaling we can look at existing correlations in the data to identify features which may be more useful for the classification task. We can compare the distributions of features for each class label and also look at the correlations between individual features. For this analysis we will use a Python package called [seaborn](https://seaborn.pydata.org/). Seaborn is built upon matplotlib providing convenience functions for creating useful visualisations.\n",
    "\n",
    "Here we will use the [`pairplot()` function](https://seaborn.pydata.org/generated/seaborn.pairplot.html) which plots pairwise relationships in the DataFrame:\n",
    "\n",
    " - We use `dataset.frame` as it contains all features and also the target column\n",
    " - The hue argument specifys which column to use for colouring the data\n",
    " - the vars argument specifies which columns to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns  # import seaborn\n",
    "\n",
    "# we will restrict the columns to ones that contain the 'mean' features\n",
    "cols =  [x for x in features.columns if 'mean' in x]\n",
    "\n",
    "# Now we can construct the plot (may take a few seconds)\n",
    "sns.pairplot(dataset.frame, hue='target', vars=cols);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot is pretty large! But reveals some interesting insights into our data. From the feature distributions (diagonal) we can see that the geometry based features (radius, perimeter, area etc.) vary more greatly per class than other features such as texture and smoothness. We can also see that features such as radius and perimeter are highly correlated (obviously!). What else can you decipher from the above plot?\n",
    "\n",
    "Sometimes having so many plots on one page can be hard to interpret, instead we can use a heatmap to plot the correlation between features. Pandas has a method `.corr()` which can calculate the correlation which we can feed into the seaborn `heatmap()` [function](https://seaborn.pydata.org/generated/seaborn.heatmap.html?highlight=heatmap#seaborn.heatmap). Now we can also see all the features in one plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First calculate the correlation matrix\n",
    "correlation = features.corr()\n",
    "\n",
    "# Plot the result (here we set up a matplotlib figure to specify the size)\n",
    "plt.figure(figsize=(16,12)) \n",
    "sns.heatmap(correlation);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What correlations can you pick out? Do they make sense?\n",
    "\n",
    "Say we have identified that 'mean concave points' may be a useful feature based on the distributions we have plotted. We can check the correlation of this feature with the class label. One could use the [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) to test this since our target value is dichotomous. This correlation is also known as a point-biserial correlation coefficient (real vs dichotomous categorical). The [SciPy](https://scipy.org/) package has statistical tools we can utilise: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "col = 'mean concave points'\n",
    "result = pearsonr(features[col], target)\n",
    "\n",
    "print('Correlation:', result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there is a definite correlation between the 'mean concave points' and our target variable! Can you identify any other features with high correlation to the target?\n",
    "\n",
    "Why not calculate a correlation for all features vs the target and produce a visualisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# We can construct a dataframe to store our correlations:\n",
    "rankings = pd.DataFrame({\n",
    "    'feature': features.columns\n",
    "})\n",
    "\n",
    "# Calculate the pearson r-squared for each feature:\n",
    "rankings['pearsonr'] = rankings.feature.apply(lambda x: pearsonr(features[x], target)[0])\n",
    "\n",
    "# lets sort the features by pearsonr and display most negatively correlated:\n",
    "rankings.sort_values('pearsonr', inplace=True)\n",
    "rankings.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Plot the result\n",
    "ax = rankings['pearsonr'].plot.barh(figsize=(12, 10))\n",
    "ax.set_title('Feature vs Target Pearson Correlation')\n",
    "ax.set_yticklabels(rankings.feature)\n",
    "ax.set_xlabel('Correlation');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some useful features and some perhaps not so useful features. Note that negative correlations are just as useful as positive correlations. We could potentially reduce the number of features we use for training to reduce the complexity of the model. In this notebook we will use all the features and check whether the model agrees with the correlation based ranking.\n",
    "\n",
    "-----\n",
    "\n",
    "## Dimensionality Reduction\n",
    "\n",
    "The number of input variables or 'features' in a dataset is referred to as its dimensionality. Dimensionality reduction techniques are a group of algorithms that are able to reduce the dimensionality of a dataset while preserving its information content. The higher the dimesnionality of a dataset often the more challenging it is to construct a model. This problem is known as 'the curse of dimesnionality'. \n",
    "\n",
    "While using dimensionality reduction techniques can be used to simplify your features before input into a machine learning algorithm, it also a great way to help visualize data with more than three dimensions. These sorts of visualisations can help us decide the next steps of our predictive modelling project, such as which model might be applicable.   \n",
    "\n",
    "**Principal Component Analysis (PCA)**\n",
    "\n",
    "[Principal component analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis) is by far the most popular tool for dimensionality reduction. PCA is the process of computing the principal components and using them to perform a change of basis on the data. PCA considers the most informative dimensions of the data called principal components. These components capture most of the variation in the original dataset. PCA can be considered an unsupervised machine learning technique. \n",
    "\n",
    "The mathemtics of PCA is beyond the scope of this notebook, but those interested can take a look at the [wikipedia article](https://en.wikipedia.org/wiki/Principal_component_analysis).\n",
    "\n",
    "**Visualisation**\n",
    "\n",
    "Depicting data in two or three dimesnsions is easy with x- y- and z-axes, however depicting things in futher dimensions is impossible. PCA is able to reduce the dimensions to two or three so that we can visualise the data effectively. Remeber that earlier we mentioned that some algorithms are sensitive to scale? PCA is one of these algorithms, so lets begin by scaling the data using scikit-learns [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) which standardizes scale by removing the mean and scaling to unit variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# First setup the scaler:\n",
    "scaler = StandardScaler() \n",
    "\n",
    "# Now scale the features (now a NumPy Array):\n",
    "scaled_X = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's transform the scaled data with PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# We will reduce the dimensions to 2:\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Transform the data\n",
    "Xt = pca.fit_transform(scaled_X)\n",
    "\n",
    "# Check the output shape!\n",
    "print('New Shape:', Xt.shape)\n",
    "\n",
    "# We can save these components to a new DataFrame\n",
    "components = pd.DataFrame(Xt, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Add the targets to help with plotting\n",
    "components['Target'] = target\n",
    "components.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice way to visualise this is to use a simple scatter plot using seaborn `scatterplot()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert binary labels to text:\n",
    "components['Target'] = components.Target.apply(lambda x: 'Benign' if x == 0 else 'Malignant')\n",
    "\n",
    "# Create scatter plot:\n",
    "plt.figure(figsize=(10, 8)) # Make figure larger!\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='Target', data=components);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like our classes look quite easily seperable using the features we have! This is good news for our predictive modelling. We can be quite confident that a machine learning method will perform well. If we were to use features from PCA to train a model how do we decide how many components to use? We could simply look at the cumulative sum of the explained variance ratio! Don't worry if you do not understand the next piece of code, the plot is more interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "pca = PCA().fit(scaled_X)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see easily now that ~6 components contain ~90% of the entire variance! Maybe this would be a good place to start if using these components as input to a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "​"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
