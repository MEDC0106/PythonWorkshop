{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEDC0106: Bioinformatics in Applied Biomedical Science\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../../resources/static/Banner.png\" alt=\"MEDC0106 Banner\" width=\"90%\"/>\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "---------------------------------------------------------------\n",
    "\n",
    "# 07 - Introduction to Predictive Modelling\n",
    "\n",
    "*Written by:* Oliver Scott\n",
    "\n",
    "**This notebook provides a general introduction to predictive modelling using scikit-learn.**\n",
    "\n",
    "Do not be afraid to make changes to the code cells to explore how things work!\n",
    "\n",
    "-----\n",
    "\n",
    "### What is Predictive Modelling?\n",
    "\n",
    "In short, predictive modelling is the application of statistical models to predict future outcomes. Typically a predictive model includes some form of machine learning algorithm, which is trained using existing observations in order to make a prediction given new observations. Predictive modelling can be generalised into two main classes; regression and classification.\n",
    "\n",
    "**Regression:**\n",
    "\n",
    "Regression models are based on the analysis of relationships between a dependent variable ('outcome'/'response' variable) and one or more independent variables ('predictors'/'features'), e.g. predicting the output of a biological system under different conditions. \n",
    "\n",
    "**Classification:**\n",
    "\n",
    "Classification models aim to assign discrete class labels to a set of independent variables rather than a continuous value as in regression. For example a classification model could be used to diagnose a condition (or not) based on measurements such as gene expression.\n",
    "\n",
    "To keep it simple, in this notebook we will focus on a supervised classification objective, predicting predefined class labels for a set of observations.\n",
    "\n",
    "### Supervised?\n",
    "\n",
    "Classification tasks can be further grouped into two main categories: supervised an unsupervised. In supervised learning, we know the the class labels for training data *a priori* hence we can use this knowledge to 'supervise' the models learning process. the following image illustrates a classification task for samples with two random variables (x1, x2), where the class is indicated by the colour and the dotted line represents the (linear) decision boundary used to define two decision regions. New observations will be assigned to a respective class depending on in which decision region they will fall into. We can make the assumption that our model given unseen observations will not be be completely accurate miss-classifying some percentage of input samples.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://scipython.com/static/media/uploads/blog/logistic_regression/decision-boundary.png\" alt=\"Desicion boundry\" width=\"30%\"/>\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "In contrast to supervised classification, unsupervised approaches are used when the labels for a set of observations are not known *a priori* and must be inferred from the observations themselves. Typically an unsupervised algorithm consists of some form of 'clustering' algorithm, grouping data into clusters (groups) based on some form of distance (or similarity) measurement. In this notebook we will run through a basic pipeline for constructing a supervised classification model.\n",
    "\n",
    "### What is scikit-learn?\n",
    "\n",
    "[scikit-learn](https://scikit-learn.org/stable/) (sklearn) is the most popular and robust Python package for machine learning. It provides a selection of efficient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction. The full set of features can be seen at the [projects website](https://scikit-learn.org/stable/). In this notebook we will be utilising scikit-learn to construct a supervised classification model.\n",
    "\n",
    "-----\n",
    "\n",
    "# Contents\n",
    "\n",
    "1. [Objective](#Objective)\n",
    "2. [Data Analysis](#Data-Analysis)\n",
    "3. \n",
    "\n",
    "-----\n",
    "\n",
    "#### Extra Resources:\n",
    "\n",
    "- [scikit-learn documentation](https://scikit-learn.org/stable/)\n",
    "\n",
    "-----\n",
    "\n",
    "#### References:\n",
    "\n",
    "- [scikit-learn documentation](https://scikit-learn.org/stable/)\n",
    "\n",
    "-----\n",
    "\n",
    "## Objective\n",
    "\n",
    "As mentioned previously, in this notebook we will be learning how to construct a supervised classification model. Specifically we will be using the 'Breast Cancer Wisconsin (Diagnostic) dataset' created by  Dr. William H. Wolberg, physician at the University Of Wisconsin Hospital at Madison, Wisconsin, USA. The features are \n",
    "computed from an image of a fine needdle aspirate (FNA) of a breast mass. A computer software 'Xcyt' was used to describe characteristics of the cell neuclei present in the image. The program constructs 10 features using a curve-fitting algorithm and then calculates the mean value, extreme value and standard error for each feature, resulting in 30 real-valued features. The process is described in [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://jithinjk.github.io/blog/images/histo/pcam.png\" alt=\"breastcancer\" width=\"100%\"/>\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "[image source](https://jithinjk.github.io/blog/images/histo/pcam.png)\n",
    "\n",
    "**Attributes:**\n",
    "\n",
    "Target:\n",
    "\n",
    "- Malignant or Benign\n",
    "\n",
    "Features:\n",
    "\n",
    "- Radius (mean of distances from center to points on the perimeter)\n",
    "- Texture (standard deviation of gray-scale values)\n",
    "- Perimeter\n",
    "- Area\n",
    "- Smoothness (local variation in radius lengths)\n",
    "- Compactness (perimeter^2 / area - 1.0)\n",
    "- Concavity (severity of concave portions of the contour)\n",
    "- Concave points (number of concave portions of the contour)\n",
    "- Symmetry\n",
    "- Fractal dimension (“coastline approximation” - 1)\n",
    "\n",
    "The mean, standard error and “worst” or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n",
    "\n",
    "Class distribution: 357 benign, 212 malignant\n",
    "\n",
    "**Objective**\n",
    "\n",
    "Our objective is to construct a model capable of classifying wether the breast sample is either malignant or benign using the given features and a supervised classification algortithm.\n",
    "\n",
    "-----\n",
    "\n",
    "## Data Analysis\n",
    "\n",
    "Before we even think about building a model we need to understand the data we have been given/collected. Probably the most important step is actually getting the data into Python. Luckily scikit-learn provides a method for retrieving this paticular dataset. scikit-learn is imported with the name sklearn (hyphens `-` are not allowed in Python package/module names):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets  # import the datasets module\n",
    "\n",
    "# Download the dataset:\n",
    "dataset = datasets.load_breast_cancer(as_frame=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset object contains some useful attributes to get some intial information about the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the targets in the dataset\n",
    "print('Targets:', dataset.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the names of the features in the dataset\n",
    "print('Features:', dataset.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn dataset also contains the actual data as a Pandas `DataFrame` (features) and a Pandas `Series` (target). Let's take a look at these and assign them to variables to use later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = dataset.target  # Our class labels\n",
    "target.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = dataset.data  # Our features\n",
    "features.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the targets are binary encoded [0, 1] lets check how many there are of each label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.value_counts()  # remember this from the last notebook?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there are 212 examples of class 0 'malignant' and 357 examples of class 1 'benign'. We can also plot this to get a nice visual representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add this 'Jupyter magic' to display plots in the notebook.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Construct the plot:\n",
    "target.value_counts().plot.bar()\n",
    "\n",
    "# Set the axis labels:\n",
    "plt.xlabel('Target classification')\n",
    "plt.ylabel('Count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets take a look at the features, remeber that we can use `.info()` and `.describe()` to get some nice global summaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the dataset is nice and clean, no null values! All of the features are also real-valued, saving us from some extra work. When we have discrete features we will need to use a method to 'encode' these features into numerical values.\n",
    "\n",
    "From the distributions of our data we can see that each feature occupies a different value range. Our dataset may benefit from some normalisation.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
