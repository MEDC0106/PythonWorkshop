{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEDC0106: Bioinformatics in Applied Biomedical Science\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../../resources/static/Banner.png\" alt=\"MEDC0106 Banner\" width=\"90%\"/>\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "---------------------------------------------------------------\n",
    "\n",
    "# 05 - Introduction to Pandas\n",
    "\n",
    "*Written by:* Oliver Scott\n",
    "\n",
    "**This notebook provides a general introduction to Pandas.**\n",
    "\n",
    "Do not be afraid to make changes to the code cells to explore how things work!\n",
    "\n",
    "### What is Pandas?\n",
    "\n",
    "**[Pandas](https://pandas.pydata.org/)** is a Python package for data analysis, providing functions for analysing, cleaning and manipulating data. Pandas is probably one of the most important tools for data scientists and is the backbone of most data science projects using Python.\n",
    "\n",
    "Pandas is built on top of NumPy, hence the Numpy structure is used a lot in the Pandas interface. Data manipulation often prefaces further analysis using other Python packages such as statistical analysis using [SciPy](https://www.scipy.org/), visualisation using tools such as [Matplotlib](https://matplotlib.org/) and machine learning using [scikit-learn](https://scikit-learn.org/stable/). These tools and others make up the Python scientific stack and are essential to learn for a career in informatics or data-science. To be effective in pandas it is essential to have a good grasp of the core concepts in Python (these concepts are outlined in the first session) along with some familiarity with NumPy. If you get lost with some concepts it might be a good idea to take a look through the previous material across the sessions.\n",
    "\n",
    "In this notebook we will learn the basics of Pandas. Pandas is a huge package and is deserving of an entire lecture series itself, so here we will learn tyhe fundamentals from which you will be able to build upon if you want to learn more.\n",
    "\n",
    "-----\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [The Basics](#The-Basics)\n",
    "2. [Creating DataFrames](#Creating-DataFrames)\n",
    "3. [Reading Data](#Reading-Data)\n",
    "4. [Essential Operations](#Essential-Operations)\n",
    "5. [Slicing and Selecting](#Slicing-and-Selecting)\n",
    "\n",
    "-----\n",
    "\n",
    "#### Extra Resources:\n",
    "\n",
    "- [Pandas Getting Started Guide](https://pandas.pydata.org/pandas-docs/stable/getting_started/index.html)\n",
    "\n",
    "-----\n",
    "\n",
    "#### References:\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "## The Basics\n",
    "\n",
    "Importing Pandas is no different to any other package/module. Pandas users often use the `pd` alias to keep code clean:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "s = pd.Series([1.0, 2.0, 3.0, 5.0])\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core Components\n",
    "\n",
    "Pandas has two core components, the `Series` and the `DataFrame`.\n",
    "\n",
    "The `Series` can be imagined as a single column in a data table, whereas the `DataFrame` can be imagined as a full data table made up of multiple `Series`. Both types have a similar interface allowing a user to perform similar operations. DataFrames are similar to spreadsheets that you may have interacted with in software such as Excel. DataFrames are often faster, easier to use and more powerful than spreadsheets.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.datasciencemadesimple.com/wp-content/uploads/2020/05/create-series-in-python-pandas-0.png?ezimgfmt=rs%3Adevice%2Frscb1-1\" alt=\"Pandas DataFrame\" width=\"70%\"/>\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "[Image source](https://www.datasciencemadesimple.com/wp-content/uploads/2020/05/create-series-in-python-pandas-0.png?ezimgfmt=rs%3Adevice%2Frscb1-1)\n",
    "\n",
    "-----\n",
    "\n",
    "## Creating DataFrames\n",
    "\n",
    "There are numerous ways to create a DataFrame using the Pandas package. In most cases it is likely that you will want to read in data from a paticular file, however DataFrames can also be constructed from scratch from lists, tuples, NumPy arrays or Pandas series. Probably the most simple way however is from a simple Python dictionary `dict`. Suppose we wanted to construct a table like the one below:\n",
    "\n",
    "| PatientID | Gender | Age | Outcome  |\n",
    "|-----------|--------|-----|----------|\n",
    "| 556785    | M      | 19  | Negative |\n",
    "| 998764    | F      | 38  | Positive |\n",
    "| 477822    | M      | 54  | Positive |\n",
    "| 678329    | M      | 22  | Negative |\n",
    "| 675859    | F      | 41  | Negative |\n",
    "\n",
    "We can construct this using a Python dictionary where the key corresponds to the column name and the list the data present in the rows. For this we can use the default constructor `pd.DataFrame()`. Notice how there is also an unnamed column containing the numbers 0-4, this is the **index** of each row. In fact you may also specify a custom index when contructing a dataframe; (`pd.DataFrame(data, index=['Tom', 'Joanne', 'Joe', 'Xander', 'Selena'])`) In this case the index is the names of the patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is or dictionary containing the raw data\n",
    "data = {\n",
    "    'PatientID': [556785, 998764, 477822, 678329, 675859],\n",
    "    'Gender': ['M', 'F', 'M', 'M', 'F'],\n",
    "    'Age': [19, 38, 54, 22, 41],\n",
    "    'Outcome': ['Negative', 'Poisitive', 'Positive', 'Negative', 'Negative']\n",
    "}\n",
    "\n",
    "# We can now construct a DataFrame like so:\n",
    "df = pd.DataFrame(data)\n",
    "df  # show the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often you will be working with very large tables of data making it impractical to view the whole table. Pandas provides a method `.head()` to display the first few n items or `.tail()` for the last few:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first three rows\n",
    "df.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the last two rows\n",
    "df.tail(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing an individual column is easy using the same syntax as a Python dictionary `dict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_column = df['Gender']\n",
    "gender_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the column label is a string you may also use **dot-syntax** to access the column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_column = df.Age\n",
    "age_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data\n",
    "\n",
    "Reading and writing data from/to files in multiple formats is an essential part of the data analysis pipeline. Pandas can read data from file including; CSV, JSON, Excel, SQL and [many more](https://pandas.pydata.org/pandas-docs/stable/reference/io.html).\n",
    "\n",
    "In the folder `data` we have provided a dataset downloaded from the [UK government](https://coronavirus.data.gov.uk/details/cases?areaType=overview&areaName=United%20Kingdom) detailing the number of reported positive COVID-19 test results in the United Kingdom by date reported (up to Oct-31-21). The file is in the CSV format and can be read using Pandas with the function `.read_csv()`: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data_path = './data/data_2021-Oct-31.csv'  # This is the path to our data\n",
    "\n",
    "cv_data = pd.read_csv(cv_data_path)\n",
    "cv_data.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also easily write this DataFrame to a new CSV file using the method `df.to_csv()`:\n",
    "\n",
    "```python\n",
    "cv_data.to_csv('./data/coronavirus_testing_results.csv')\n",
    "```\n",
    "\n",
    "Give it a go. Maybe also saving to a different [format](https://pandas.pydata.org/pandas-docs/stable/reference/io.html)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essential Operations\n",
    "\n",
    "Now that we have loaded some data into a `DataFrame` we can perform operations for performing analysis. Typically once you have loaded some data you should view your data to make sure that it looks correct and to get an idea of what values you will be dealing with. Since we have already coovered visualising the data using `.head()`/`.tail()`, the next function you should probbaly run is `.info()` which provide essential details about your dataset including the number of rows/columns  the number of none-null values (None), what type of data is in each column and how much memory the data is taking up:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have 6 columns of which four are of type `object` (this could be something like a string) and two that are `int64` (integers) (these types correspond the the types used in NumPy). The info also tells us that we have 2466 non-null values and no null-values in this case. Knowing the datatype of ourt columns is very important as it will determine what operations we can perform on each column (we wouldnt want to calculate the mean of a column containing strings). Just like NumPy you can also use `.shape` to see the number of (rows/columns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing duplicate data\n",
    "\n",
    "Often input data is noisy and needs cleaning up before we do any further analysis. It is often the case that data contains duplicated rows which is not great when we are trying to do statitical analysis. Luckily Pandas has utilities for dealing with this problem easily. The data we have read does not contain any duplicated rows so we will arbritrarily create some by duplicating the data and adding it to itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated = cv_data.append(cv_data)  # here we have copied the data and added it to itself\n",
    "duplicated.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have to assign the result of the `append` to a new variable. Here we have copied the data so we wont do anything to the original DataFrame. We can now easily drop the duplicates using the `.drop_duplicates()` [method](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop_duplicates.html). It is always a good idea to look at the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop_duplicates.html) to see what other arguments these functions accept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated = duplicated.drop_duplicates()\n",
    "duplicated.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the shape is now the same as the original data. Also notice that again we assigned the result to a new variable (with the same name). This technique can get quite annoying so Pandas often offers an argument `inplace` which if we set to `True` allows pandas to perform the operation modifying the original data rather than a copy.\n",
    "\n",
    "```python\n",
    "duplicated.drop_duplicates(inplace=True)  # no need to assign to a new variable\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Null values (None)\n",
    "\n",
    "Data before cleaning commonly has missing values that you will need to deal with before further analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
