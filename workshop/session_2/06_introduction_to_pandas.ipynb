{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEDC0106: Bioinformatics in Applied Biomedical Science\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../../resources/static/Banner.png\" alt=\"MEDC0106 Banner\" width=\"90%\"/>\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "---------------------------------------------------------------\n",
    "\n",
    "# 05 - Introduction to Pandas\n",
    "\n",
    "*Written by:* Oliver Scott\n",
    "\n",
    "**This notebook provides a general introduction to Pandas.**\n",
    "\n",
    "Do not be afraid to make changes to the code cells to explore how things work!\n",
    "\n",
    "### What is Pandas?\n",
    "\n",
    "**[Pandas](https://pandas.pydata.org/)** is a Python package for data analysis, providing functions for analysing, cleaning and manipulating data. Pandas is probably one of the most important tools for data scientists and is the backbone of most data science projects using Python.\n",
    "\n",
    "Pandas is built on top of NumPy, hence the Numpy structure is used a lot in the Pandas interface. Data manipulation often prefaces further analysis using other Python packages such as statistical analysis using [SciPy](https://www.scipy.org/), visualisation using tools such as [Matplotlib](https://matplotlib.org/) and machine learning using [scikit-learn](https://scikit-learn.org/stable/). These tools and others make up the Python scientific stack and are essential to learn for a career in informatics or data-science. To be effective in pandas it is essential to have a good grasp of the core concepts in Python (these concepts are outlined in the first session) along with some familiarity with NumPy. If you get lost with some concepts it might be a good idea to take a look through the previous material across the sessions.\n",
    "\n",
    "In this notebook we will learn the basics of Pandas. Pandas is a huge package and is deserving of an entire lecture series itself, so here we will learn tyhe fundamentals from which you will be able to build upon if you want to learn more.\n",
    "\n",
    "-----\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [The Basics](#The-Basics)\n",
    "2. [Creating DataFrames](#Creating-DataFrames)\n",
    "3. [Reading Data](#Reading-Data)\n",
    "4. [Essential Operations](#Essential-Operations)\n",
    "5. [Slicing and Selecting](#Slicing-and-Selecting)\n",
    "5. [Arithmetic](#Arithmetic)\n",
    "6. [Applying Functions](#Applying-Functions)\n",
    "7. [Time-Series](#Time-Series)\n",
    "8. [Plotting](#Plotting)\n",
    "\n",
    "-----\n",
    "\n",
    "#### Extra Resources:\n",
    "\n",
    "- [Pandas Getting Started Guide](https://pandas.pydata.org/pandas-docs/stable/getting_started/index.html)\n",
    "- [RealPython-01](https://realpython.com/pandas-python-explore-dataset/)\n",
    "- [RealPython-02](https://realpython.com/pandas-dataframe/)\n",
    "\n",
    "-----\n",
    "\n",
    "#### References:\n",
    "\n",
    "- [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "- [Learn Data Science](https://www.learndatasci.com/tutorials/python-pandas-tutorial-complete-introduction-for-beginners/)\n",
    "-----\n",
    "\n",
    "## The Basics\n",
    "\n",
    "Importing Pandas is no different to any other package/module. Pandas users often use the `pd` alias to keep code clean:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "s = pd.Series([1.0, 2.0, 3.0, 5.0])\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core Components\n",
    "\n",
    "Pandas has two core components, the `Series` and the `DataFrame`.\n",
    "\n",
    "The `Series` can be imagined as a single column in a data table, whereas the `DataFrame` can be imagined as a full data table made up of multiple `Series`. Both types have a similar interface allowing a user to perform similar operations. DataFrames are similar to spreadsheets that you may have interacted with in software such as Excel. DataFrames are often faster, easier to use and more powerful than spreadsheets.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.datasciencemadesimple.com/wp-content/uploads/2020/05/create-series-in-python-pandas-0.png?ezimgfmt=rs%3Adevice%2Frscb1-1\" alt=\"Pandas DataFrame\" width=\"70%\"/>\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "[Image source](https://www.datasciencemadesimple.com/wp-content/uploads/2020/05/create-series-in-python-pandas-0.png?ezimgfmt=rs%3Adevice%2Frscb1-1)\n",
    "\n",
    "-----\n",
    "\n",
    "## Creating DataFrames\n",
    "\n",
    "There are numerous ways to create a DataFrame using the Pandas package. In most cases it is likely that you will want to read in data from a paticular file, however DataFrames can also be constructed from scratch from lists, tuples, NumPy arrays or Pandas series. Probably the most simple way however is from a simple Python dictionary `dict`. Suppose we wanted to construct a table like the one below:\n",
    "\n",
    "| PatientID | Gender | Age | Outcome  |\n",
    "|-----------|--------|-----|----------|\n",
    "| 556785    | M      | 19  | Negative |\n",
    "| 998764    | F      | 38  | Positive |\n",
    "| 477822    | M      | 54  | Positive |\n",
    "| 678329    | M      | 22  | Negative |\n",
    "| 675859    | F      | 41  | Negative |\n",
    "\n",
    "We can construct this using a Python dictionary where the key corresponds to the column name and the list the data present in the rows. For this we can use the default constructor `pd.DataFrame()`. Notice how there is also an unnamed column containing the numbers 0-4, this is the **index** of each row. In fact you may also specify a custom index when contructing a dataframe; (`pd.DataFrame(data, index=['Tom', 'Joanne', 'Joe', 'Xander', 'Selena'])`) In this case the index is the names of the patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is or dictionary containing the raw data\n",
    "data = {\n",
    "    'PatientID': [556785, 998764, 477822, 678329, 675859],\n",
    "    'Gender': ['M', 'F', 'M', 'M', 'F'],\n",
    "    'Age': [19, 38, 54, 22, 41],\n",
    "    'Outcome': ['Negative', 'Poisitive', 'Positive', 'Negative', 'Negative']\n",
    "}\n",
    "\n",
    "# We can now construct a DataFrame like so:\n",
    "df = pd.DataFrame(data)\n",
    "df  # show the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often you will be working with very large tables of data making it impractical to view the whole table. Pandas provides a method `.head()` to display the first few n items or `.tail()` for the last few:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first three rows\n",
    "df.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the last two rows\n",
    "df.tail(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing an individual column is easy using the same syntax as a Python dictionary `dict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_column = df['Gender']\n",
    "gender_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the column label is a string you may also use **dot-syntax** to access the column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_column = df.Age\n",
    "age_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data\n",
    "\n",
    "Reading and writing data from/to files in multiple formats is an essential part of the data analysis pipeline. Pandas can read data from file including; CSV, JSON, Excel, SQL and [many more](https://pandas.pydata.org/pandas-docs/stable/reference/io.html).\n",
    "\n",
    "In the folder `data` we have provided a dataset downloaded from the [UK government](https://coronavirus.data.gov.uk/details/cases?areaType=overview&areaName=United%20Kingdom) detailing the number of reported positive COVID-19 test results in the United Kingdom by date reported (up to Oct-31-21). The file is in the CSV format and can be read using Pandas with the function `.read_csv()`: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data_path = './data/data_2021-Oct-31.csv'  # This is the path to our data\n",
    "\n",
    "cv_data = pd.read_csv(cv_data_path)\n",
    "cv_data.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also easily write this DataFrame to a new CSV file using the method `df.to_csv()`:\n",
    "\n",
    "```python\n",
    "cv_data.to_csv('./data/coronavirus_testing_results.csv')\n",
    "```\n",
    "\n",
    "Give it a go. Maybe also saving to a different [format](https://pandas.pydata.org/pandas-docs/stable/reference/io.html)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essential Operations\n",
    "\n",
    "Now that we have loaded some data into a `DataFrame` we can perform operations for performing analysis. Typically once you have loaded some data you should view your data to make sure that it looks correct and to get an idea of what values you will be dealing with. Since we have already coovered visualising the data using `.head()`/`.tail()`, the next function you should probbaly run is `.info()` which provide essential details about your dataset including the number of rows/columns  the number of none-null values (None), what type of data is in each column and how much memory the data is taking up:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have 6 columns of which four are of type `object` (this could be something like a string) and two that are `int64` (integers) (these types correspond the the types used in NumPy). The info also tells us that we have 2466 non-null values and no null-values in this case. Knowing the datatype of ourt columns is very important as it will determine what operations we can perform on each column (we wouldnt want to calculate the mean of a column containing strings). Just like NumPy you can also use `.shape` to see the number of (rows/columns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing duplicate data\n",
    "\n",
    "Often input data is noisy and needs cleaning up before we do any further analysis. It is often the case that data contains duplicated rows which is not great when we are trying to do statitical analysis. Luckily Pandas has utilities for dealing with this problem easily. The data we have read does not contain any duplicated rows so we will arbritrarily create some by duplicating the data and adding it to itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated = cv_data.append(cv_data)  # here we have copied the data and added it to itself\n",
    "duplicated.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have to assign the result of the `append` to a new variable. Here we have copied the data so we wont do anything to the original DataFrame. We can now easily drop the duplicates using the `.drop_duplicates()` [method](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop_duplicates.html). It is always a good idea to look at the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop_duplicates.html) to see what other arguments these functions accept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated = duplicated.drop_duplicates()\n",
    "duplicated.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the shape is now the same as the original data. Also notice that again we assigned the result to a new variable (with the same name). This technique can get quite annoying so Pandas often offers an argument `inplace` which if we set to `True` allows pandas to perform the operation modifying the original data rather than a copy.\n",
    "\n",
    "```python\n",
    "duplicated.drop_duplicates(inplace=True)  # no need to assign to a new variable\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Null values (None)\n",
    "\n",
    "Data before cleaning commonly has missing values that you will need to deal with before further analysis. Missing values are represented by `None` or `np.nan`. There is usually two options to dealing with missing values:\n",
    "\n",
    "1. Remove all rows with missing data\n",
    "2. Imputing the missing values\n",
    "\n",
    "In this tutorial we will stick to the first case.\n",
    "\n",
    "Again as our data is nice and clean it contains no null values so for this example we will inject a new column containing some null values, let's do this first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_rows = cv_data.shape[0]\n",
    "# p is for weighting the choice here it is more likely to choose 1 than None\n",
    "null_containing_data = np.random.choice([None, 1], n_rows, p=[0.2, 0.8])\n",
    "null_containing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add a row to the data containing our constructed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data['RandomData'] = null_containing_data  # make a colum called 'RandomData'\n",
    "cv_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see now that we have null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the number of null values in each column using `.isnull()`. This returns a dataframe with boolean columns where `True` indicated a null value. We can then use `.sum()` to count the number of `True` values in each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing data analysis you often have to make the choice to remove missing values or impute them in some way. Removing data is only really recommended if the number of missing data points is relatively small. To remove null values you can simply use the [method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html) `.dropna()`. This operation will remove any row with at least 1 null value, returning a new DataFrame unless you specified inplace. Instead of dropping rows we could instead drop columns with null values by changing the axis of operation. Columns are represented by `axis=1` (The axes are defined in the same way as NumPy!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets remove rows with null values\n",
    "remove_rows = cv_data.dropna()\n",
    "remove_rows.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_rows.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets change the axis and remove the colum we injected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.dropna(axis=1, inplace=True)  # We can do it inplace since we do not care about this column\n",
    "cv_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding Data\n",
    "\n",
    "Now that your data is clean(er) than when we started, it is time to do some basic stats to understand the data that we have in each column. This may help inform us how to continue with our analysis and maybe how to plot the data. Pandas provides us with an easy way to get a quick summary of the distribution of our continuous variables `.describe()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do the same for categorical columns but we will have to do it seperately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.areaName.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows us that in this dataset there are four unique area names with 'England' being the most frequent with a frequency of 640. We can also check the unique values using the `.unique()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.areaName.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the dataset contains data for:\n",
    "\n",
    "- England\n",
    "- Northern Ireland\n",
    "- Scotland\n",
    "- Wales\n",
    "\n",
    "But how many times are these values recorded? We can use the method `.value_counts()` to find out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.areaName.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing and Selecting\n",
    "\n",
    "In the previous section we saw how to produce summaries of the entire data which is useful however, sometimes we will want to perform analysis on certain subsets of data. We have already seen how to extract a column of data using square brackets and dot-syntax `df['col'] / df.col` and now we will dive deeper into the Pandas selction language. When selecting parts of a DataFrame we may be returning either a `DataFrame` or a `Series`, it is important to know which so that yopu use the correct syntax.\n",
    "\n",
    "\n",
    "#### Selecting by Column(s)\n",
    "\n",
    "Using the square-bracket syntax we mentioned previously will return a Pandas `Series`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cv_data['areaName'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to access it as a dataframe you can supply the column name as a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cv_data[['areaName']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding another column to our selection is a simple as adding another column name to the list. Obviosuly inh this case our code will return a `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = cv_data[['areaName', 'areaCode']]\n",
    "selection.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting by rows\n",
    "\n",
    "Selecting rows is a little trickier with two methods:\n",
    "\n",
    "- `.loc`:  locate by name\n",
    "- `.iloc`: locate by numerical index\n",
    "\n",
    "Considering that our data has a numerical index it makes sense for us to use `.iloc`. If our data has an index using strings `.loc` would be the correct solution if we want to select using the string. Of course `.iloc` will also work returning the data at the numerical position instead of the name.\n",
    "\n",
    "Both methods are similar to indexing lists or NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.loc[222]  # Return the row with index 222"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Pandas is backed by NumPy we can also use slices to select a range of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.loc[222:226]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional Selections\n",
    "\n",
    "Selectind data by index can be useful, but if we do not know what dat the indexes correspond too this can be limiting. Perhaps we are only interested in the data from Wales, we can use conditional selections to make informed selections.\n",
    "\n",
    "Pandas like numpy can be indexed using a boolean array/Series/DataFrame generated using a conditional expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = cv_data['areaName'] == 'Wales'  # A boolean Series\n",
    "ind.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this boolean Series we can index the DataFrame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wales_data = cv_data[ind]\n",
    "wales_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simplify this quite nicely into a one line expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wales_data = cv_data[cv_data['areaName'] == 'Wales']\n",
    "wales_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we can apply this to numerical columns also:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows where reported positives is less than 100\n",
    "cv_data[cv_data['newCasesByPublishDate'] < 100].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaining conditional expressions allows us to create powerful selections. For this we can use the logical operators `|` and `&`. Remeber to put seperate conditions in brackets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count dates in england with reported positive results > 10,000\n",
    "cv_data[(cv_data['areaName'] == 'England') & (cv_data['newCasesByPublishDate'] > 10000)].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arithmetic\n",
    "\n",
    "Basic arithmentic operations can be applied in the same way as NumPy arrays, so we will quickly brush over it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.newCasesByPublishDate / 100   # divide a column by 100 return a Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also perform arithmetic between columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.newCasesByPublishDate + cv_data.cumCasesByPublishDate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can insert a new column with the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data['Rubbish'] = cv_data.newCasesByPublishDate * 0.3 / cv_data.cumCasesByPublishDate\n",
    "cv_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas also provides some handy utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_data.newCasesByPublishDate.mean())\n",
    "print(cv_data.newCasesByPublishDate.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Functions\n",
    "\n",
    "While it is possible to iterate over a DataFrame/Series like a NumPy array, it is slow in Python so instead we can use the `.apply()` function to apply a function to each element in a column or across columns. We can also save this result to a new column. Let's create an arbritary function that we can apply to the data we have:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_cases(x):\n",
    "    if x >= 10000:\n",
    "        return 'High'\n",
    "    elif x <= 200:\n",
    "        return 'Low'\n",
    "    else:\n",
    "        return 'Medium'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function categorises a case count into arbritarty categories: 'High', 'Medium' and 'Low'. Now we can apply this to the column 'newCasesByPublishDate':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data['Category'] = cv_data['newCasesByPublishDate'].apply(categorize_cases)\n",
    "cv_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users often will use anonymous functions instead of defining an explicit function like above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data['newCategory'] = cv_data['newCasesByPublishDate'].apply(lambda x: 'Red' if x >= 20000 else 'Amber')\n",
    "cv_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-Series\n",
    "\n",
    "Some of you may have notices that one of the columns contains dates as a string (object). This isnt paticularly useful to us in this form. Pandas however has a datetime type which we can use to make some more intelligent selections based on time spans. First we need to tell pandas that our column is a datetime column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data['date'] = pd.to_datetime(cv_data['date'])\n",
    "cv_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the date in this form we can make selections within time ranges using the `.between()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets select data between the 20th and the 30th October 2021 and restrict it to England\n",
    "selection = cv_data[(cv_data.date.between('2021-10-20','2021-10-30')) & (cv_data.areaName == 'England')]\n",
    "selection.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with time-series data is even more powerful if we use the time as our index. Lets first only consider 'Scotland' in our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scotland_data = pd.DataFrame(cv_data[cv_data.areaName == 'Scotland'])  # also copy into a new DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can set the index of the 'scotland_data' DataFrame as the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scotland_data.set_index('date', inplace=True)\n",
    "scotland_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we cxan simply use slicing to select a data range with `.loc`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scotland_data.loc['2021-10-30':'2021-10-20']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can resample time-series data into different intervals and get a mean value for that interval. Below we resmaple the data into 10-day intervals and calculate the mean of 'newCasesByPublishDate':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scotland_data.resample(rule='10d')['newCasesByPublishDate'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of mean you could use other functions such as `min()`, `max()`, `sum()` etc. Indeed you can also calculate a rolling statistic using `.rolling()` and a window size. Here we will calculate a rolling average using a three day window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scotland_data['rollingAvgThreeDay'] = scotland_data.rolling(3)['newCasesByPublishDate'].mean()\n",
    "scotland_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "Pandas allows the visualisation of data in DataFrames/Series interfacing with the plotting package [matplotlib](https://matplotlib.org/). Displaying the plots will first require that import matplotlib:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also add this 'Jupyter magic' to display plots in the notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now creating a plot with pandas is as simple as calling `.plot()` on some selected data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scotland_data.newCasesByPublishDate.plot();  # We also add the semicolon when plotting in Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have also achieved the same result using the syntax:\n",
    "\n",
    "```python\n",
    "scotland_data.newCasesByPublishDate.plot.line()\n",
    "```\n",
    "\n",
    "These plotting functions also have many [arguments](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) which you can specify to tune the look of your plots. Thes arguments are passed to the underlying matplotlib methods. We can also specify other types of plot. For example we could visualise the data as a box plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a time window (1-month)\n",
    "window = scotland_data['2021-10-30':'2021-09-30']\n",
    "\n",
    "window.newCasesByPublishDate.plot.box();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about we plot the raw data along with a 10 day rolling average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scotland_data['rollingAvgTenDay'] = scotland_data.rolling(10)['newCasesByPublishDate'].mean()\n",
    "\n",
    "scotland_data.newCasesByPublishDate.plot(figsize=(12, 8))  # also specify the size!\n",
    "scotland_data.rollingAvgTenDay.plot()\n",
    "plt.legend();  # We can also add a legend using matplotlib!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save figures using `.savefig()`, check the data directory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = scotland_data.newCasesByPublishDate.plot(figsize=(12, 8)).get_figure()\n",
    "figure.savefig('./data/Scotland_2021-Oct-31.png');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Cleaning, analysing, manipulating and visualizing data is an essential skill for an informatician/data-scientist. In fact 80% of a data-scientists job is cleaning data for analysis. As Pandas is such a widley used tool if you want to know more there are hundreds of resources online to help you learn.\n",
    "\n",
    "Feel free to add more code cells and experiment with the concepts you have learnt.\n",
    "\n",
    "If you want to know more there are some extra resources from external sources linked in the beginning section. You can click the link below to go back to the top.\n",
    "\n",
    "Click [here](#Contents) to go back to the contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
