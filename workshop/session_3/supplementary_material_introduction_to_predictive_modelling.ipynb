{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### MEDC0106: Bioinformatics in Applied Biomedical Science\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../../resources/static/Banner.png\" alt=\"MEDC0106 Banner\" width=\"90%\"/>\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "---------------------------------------------------------------\n",
    "\n",
    "# 10 - Introduction to Predictive Modelling\n",
    "\n",
    "*Written by:* Oliver Scott\n",
    "\n",
    "**This notebook provides a general introduction to predictive modelling using scikit-learn.**\n",
    "\n",
    "Do not be afraid to make changes to the code cells to explore how things work!\n",
    "\n",
    "-----\n",
    "\n",
    "### What is Predictive Modelling?\n",
    "\n",
    "In short, predictive modelling is the application of statistical models to predict future outcomes. Typically a predictive model includes some form of machine learning algorithm, which is trained using existing observations in order to make a prediction given new observations. Predictive modelling can be generalised into two main classes; regression and classification.\n",
    "\n",
    "**Regression:**\n",
    "\n",
    "Regression models are based on the analysis of relationships between a dependent variable ('outcome'/'response' variable) and one or more independent variables ('predictors'/'features'), e.g. predicting the output of a biological system under different conditions. \n",
    "\n",
    "**Classification:**\n",
    "\n",
    "Classification models aim to assign discrete class labels to a set of independent variables rather than a continuous value as in regression. For example a classification model could be used to diagnose a condition (or not) based on measurements such as gene expression.\n",
    "\n",
    "To keep it simple, in this notebook we will focus on a supervised classification objective, predicting predefined class labels for a set of observations.\n",
    "\n",
    "### Supervised?\n",
    "\n",
    "Classification tasks can be further grouped into two main categories: supervised an unsupervised. In supervised learning, we know the the class labels for training data *a priori* hence we can use this knowledge to 'supervise' the models learning process. the following image illustrates a classification task for samples with two random variables (x1, x2), where the class is indicated by the colour and the dotted line represents the (linear) decision boundary used to define two decision regions. New observations will be assigned to a respective class depending on in which decision region they will fall into. We can make the assumption that our model given unseen observations will not be be completely accurate miss-classifying some percentage of input samples.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://scipython.com/static/media/uploads/blog/logistic_regression/decision-boundary.png\" alt=\"Desicion boundry\" width=\"30%\"/>\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "In contrast to supervised classification, unsupervised approaches are used when the labels for a set of observations are not known *a priori* and must be inferred from the observations themselves. Typically an unsupervised algorithm consists of some form of 'clustering' algorithm, grouping data into clusters (groups) based on some form of distance (or similarity) measurement. In this notebook we will run through a basic pipeline for constructing a supervised classification model.\n",
    "\n",
    "### What is scikit-learn?\n",
    "\n",
    "[scikit-learn](https://scikit-learn.org/stable/) (sklearn) is the most popular and robust Python package for machine learning. It provides a selection of efficient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction. The full set of features can be seen at the [projects website](https://scikit-learn.org/stable/). In this notebook we will be utilising scikit-learn to construct a supervised classification model.\n",
    "\n",
    "-----\n",
    "\n",
    "# Contents\n",
    "\n",
    "1. [Objective](#Objective)\n",
    "2. [Data Analysis](#Data-Analysis)\n",
    "3. [Dimensionality Reduction](#Dimesnionality-Reduction)\n",
    "4. [Train/Test Splits](#Train/Test-Splits)\n",
    "4. [Training Models](#Training-Models)\n",
    "5. [Model Evaluation](#Model-Evaluation)\n",
    "6. [Feature Importance](#Feature-Importance)\n",
    "7. [Discussion](#Discussion)\n",
    "\n",
    "-----\n",
    "\n",
    "#### Extra Resources:\n",
    "\n",
    "- [scikit-learn documentation](https://scikit-learn.org/stable/)\n",
    "\n",
    "-----\n",
    "\n",
    "#### References:\n",
    "\n",
    "- [scikit-learn documentation](https://scikit-learn.org/stable/)\n",
    "\n",
    "-----\n",
    "\n",
    "## Objective\n",
    "\n",
    "As mentioned previously, in this notebook we will be learning how to construct a supervised classification model. Specifically we will be using the 'Breast Cancer Wisconsin (Diagnostic) dataset' created by  Dr. William H. Wolberg, physician at the University Of Wisconsin Hospital at Madison, Wisconsin, USA. The features are \n",
    "computed from an image of a fine needle aspirate (FNA) of a breast mass. A computer software 'Xcyt' was used to describe characteristics of the cell neuclei present in the image. The program constructs 10 features using a curve-fitting algorithm and then calculates the mean value, extreme value and standard error for each feature, resulting in 30 real-valued features. The process is described in [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://jithinjk.github.io/blog/images/histo/pcam.png\" alt=\"breastcancer\" width=\"100%\"/>\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "[image source](https://jithinjk.github.io/blog/images/histo/pcam.png)\n",
    "\n",
    "**Attributes:**\n",
    "\n",
    "Target:\n",
    "\n",
    "- Malignant or Benign\n",
    "\n",
    "Features:\n",
    "\n",
    "- Radius (mean of distances from center to points on the perimeter)\n",
    "- Texture (standard deviation of gray-scale values)\n",
    "- Perimeter\n",
    "- Area\n",
    "- Smoothness (local variation in radius lengths)\n",
    "- Compactness (perimeter^2 / area - 1.0)\n",
    "- Concavity (severity of concave portions of the contour)\n",
    "- Concave points (number of concave portions of the contour)\n",
    "- Symmetry\n",
    "- Fractal dimension (“coastline approximation” - 1)\n",
    "\n",
    "The mean, standard error and “worst” or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n",
    "\n",
    "Class distribution: 357 benign, 212 malignant\n",
    "\n",
    "**Objective**\n",
    "\n",
    "Our objective is to construct a model capable of classifying wether the breast sample is either malignant or benign using the given features and a supervised classification algortithm.\n",
    "\n",
    "**Talking Points:**\n",
    "\n",
    "- Do you think that using a machine learning model that can diagnose cancer accurately would be beneficial in a clinical setting?\n",
    "- Can you think of any reasons why a machine learning model might not be used to make such diagnoses?\n",
    "- Can you identify any other biomedical related (or not) problems that may benefit from predictive modelling?\n",
    "\n",
    "-----\n",
    "\n",
    "## Data Analysis\n",
    "\n",
    "Before we even think about building a model we need to understand the data we have been given/collected. Probably the most important step is actually getting the data into Python. Luckily scikit-learn provides a method for retrieving this paticular dataset. scikit-learn is imported with the name sklearn (hyphens `-` are not allowed in Python package/module names):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets  # import the datasets module\n",
    "\n",
    "# Download the dataset:\n",
    "dataset = datasets.load_breast_cancer(as_frame=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset object contains some useful attributes to get some intial information about the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the targets in the dataset\n",
    "print('Targets:', dataset.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the names of the features in the dataset\n",
    "print('Features:', dataset.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn dataset also contains the actual data as a Pandas `DataFrame` (features) and a Pandas `Series` (target). Let's take a look at these and assign them to variables to use later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = dataset.target  # Our class labels\n",
    "target.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = dataset.data  # Our features\n",
    "features.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the targets are binary encoded [0, 1] lets check how many there are of each label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.value_counts()  # remember this from the last notebook?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there are 212 examples of class 0 'malignant' and 357 examples of class 1 'benign'. I would prefer that the class labels were the other way around, 0 being 'benign' and 1 being 'malignant' so that it reflects a real-life scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 1 - target  # simple way to flip the labels!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the class counts to get a nice visual representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add this 'Jupyter magic' to display plots in the notebook.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Construct the plot:\n",
    "target.value_counts(sort=True).plot.bar()\n",
    "\n",
    "# Set the axis labels:\n",
    "plt.xlabel('Target classification')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['benign', 'malignant']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets take a look at the features, remember that we can use `.info()` and `.describe()` to get some nice global summaries. Check for any null values and make sure the data types make sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the dataset is nice and clean, no null values! All of the features are also real-valued, saving us from some extra work. When we have discrete features we need to use a method to 'encode' these features into numerical values. A technique called [one-hot-encoding](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/) is often used in this case.\n",
    "\n",
    "Looking at raw numbers can often be hard to interpret so visualising the distributions of features can be insightful. We can use pandas to plot these distributions using a histogram or a [density plot](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.density.html). Let's try visualise a couple feature distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram:\n",
    "features['mean radius'].plot.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a density plot:\n",
    "features['mean texture'].plot.density();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What observations can you make about the features? Try plotting some further features to make further observations.\n",
    "\n",
    "From the distributions of our data we can see that each feature occupies a different value range, although all are approximately [normally distributed](https://en.wikipedia.org/wiki/Normal_distribution). This observation tells us that we may benefit from scaling the features so that they occupy the same value range.  \n",
    "\n",
    "**Feature scaling** is an important part of data-preprocessing, as some machine learning alogithms will not function correctly without it. For example some classifiers use a distance measurement between two points. If one feature has a large range of values then this distance measurment will be influenced greatly by this feature opposed to a feature with a smaller range. Feature scaling brings all data into the same range so that each feature contributes approximatley  proportionately to the distance. Note that not all machine learning algorithms require that input data is scaled.\n",
    "\n",
    "In some cases you may want to **normalise** features so that features with strange distributions become more 'normal'. Normally distributed features tend to lead to better models because there is an approximately equal number of observations above and below the mean. Many models maker the assumption that your data is normally distributed. \n",
    "\n",
    "The difference between scaling and normalisation is that scaling changes the **range** of your data whereas normalisation changes the **shape** of the distribution. Therefore normalisation is useful when you know your data is not normally distributed and scaling is useful when your data is normally distributed but occupies different value ranges. There is a nice guide [here](https://towardsai.net/p/data-science/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff) if you need further explanation.\n",
    "\n",
    "Always look at your data!\n",
    "\n",
    "-----\n",
    "\n",
    "**Looking for correlations**\n",
    "\n",
    "Before we perform any feature scaling we can look at existing correlations in the data to identify features which may be more useful for the classification task. We can compare the distributions of features for each class label and also look at the correlations between individual features. For this analysis we will use a Python package called [seaborn](https://seaborn.pydata.org/). Seaborn is built upon matplotlib providing convenience functions for creating useful visualisations.\n",
    "\n",
    "Here we will use the [`pairplot()` function](https://seaborn.pydata.org/generated/seaborn.pairplot.html) which plots pairwise relationships in the DataFrame:\n",
    "\n",
    " - We use `dataset.frame` as it contains all features and also the target column\n",
    " - The hue argument specifys which column to use for colouring the data\n",
    " - the vars argument specifies which columns to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns  # import seaborn\n",
    "\n",
    "# we will restrict the columns to ones that contain the 'mean' features\n",
    "cols =  [x for x in features.columns if 'mean' in x]\n",
    "\n",
    "# Now we can construct the plot (may take a few seconds)\n",
    "sns.pairplot(dataset.frame, hue='target', vars=cols);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot is pretty large! But reveals some interesting insights into our data. From the feature distributions (diagonal) we can see that the geometry based features (radius, perimeter, area etc.) vary more greatly per class than other features such as texture and smoothness. We can also see that features such as radius and perimeter are highly correlated (obviously!). What else can you decipher from the above plot?\n",
    "\n",
    "Sometimes having so many plots on one page can be hard to interpret, instead we can use a heatmap to plot the correlation between features. Pandas has a method `.corr()` which can calculate the correlation which we can feed into the seaborn `heatmap()` [function](https://seaborn.pydata.org/generated/seaborn.heatmap.html?highlight=heatmap#seaborn.heatmap). Now we can also see all the features in one plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First calculate the correlation matrix\n",
    "correlation = features.corr()\n",
    "\n",
    "# Plot the result (here we set up a matplotlib figure to specify the size)\n",
    "plt.figure(figsize=(16,12)) \n",
    "sns.heatmap(correlation);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What correlations can you pick out? Do they make sense?\n",
    "\n",
    "Say we have identified that 'mean concave points' may be a useful feature based on the distributions we have plotted. We can check the correlation of this feature with the class label. One could use the [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) to test this since our target value is dichotomous. This correlation is also known as a point-biserial correlation coefficient (real vs dichotomous categorical). The [SciPy](https://scipy.org/) package has statistical tools we can utilise: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "col = 'mean concave points'\n",
    "result = pearsonr(features[col], target)\n",
    "\n",
    "print('Correlation:', result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there is a definite correlation between the 'mean concave points' and our target variable! Can you identify any other features with high correlation to the target?\n",
    "\n",
    "Why not calculate a correlation for all features vs the target and produce a visualisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# We can construct a dataframe to store our correlations:\n",
    "rankings = pd.DataFrame({\n",
    "    'feature': features.columns\n",
    "})\n",
    "\n",
    "# Calculate the pearson r-squared for each feature:\n",
    "rankings['pearsonr'] = rankings.feature.apply(lambda x: pearsonr(features[x], target)[0])\n",
    "\n",
    "# lets sort the features by pearsonr and display most negatively correlated:\n",
    "rankings.sort_values('pearsonr', inplace=True)\n",
    "rankings.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Plot the result\n",
    "ax = rankings['pearsonr'].plot.barh(figsize=(12, 10))\n",
    "ax.set_title('Feature vs Target Pearson Correlation')\n",
    "ax.set_yticklabels(rankings.feature)\n",
    "ax.set_xlabel('Correlation');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some useful features and some perhaps not so useful features. Note that negative correlations are just as useful as positive correlations. We could potentially reduce the number of features we use for training to reduce the complexity of the model. In this notebook we will use all the features and check whether the model agrees with the correlation based ranking.\n",
    "\n",
    "-----\n",
    "\n",
    "## Dimensionality Reduction\n",
    "\n",
    "The number of input variables or 'features' in a dataset is referred to as its dimensionality. Dimensionality reduction techniques are a group of algorithms that are able to reduce the dimensionality of a dataset while preserving its information content. The higher the dimesnionality of a dataset often the more challenging it is to construct a model. This problem is known as 'the curse of dimesnionality'. \n",
    "\n",
    "While using dimensionality reduction techniques can be used to simplify your features before input into a machine learning algorithm, it also a great way to help visualize data with more than three dimensions. These sorts of visualisations can help us decide the next steps of our predictive modelling project, such as which model might be applicable.   \n",
    "\n",
    "**Principal Component Analysis (PCA)**\n",
    "\n",
    "[Principal component analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis) is by far the most popular tool for dimensionality reduction. PCA is the process of computing the principal components and using them to perform a change of basis on the data. PCA considers the most informative dimensions of the data called principal components. These components capture most of the variation in the original dataset. PCA can be considered an unsupervised machine learning technique. \n",
    "\n",
    "The mathemtics of PCA is beyond the scope of this notebook, but those interested can take a look at the [wikipedia article](https://en.wikipedia.org/wiki/Principal_component_analysis).\n",
    "\n",
    "**Visualisation**\n",
    "\n",
    "Depicting data in two or three dimesnsions is easy with x- y- and z-axes, however depicting things in futher dimensions is impossible. PCA is able to reduce the dimensions to two or three so that we can visualise the data effectively. Remeber that earlier we mentioned that some algorithms are sensitive to scale? PCA is one of these algorithms, so lets begin by scaling the data using scikit-learns [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) which standardizes scale by removing the mean and scaling to unit variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# First setup the scaler:\n",
    "scaler = StandardScaler() \n",
    "\n",
    "# Now scale the features (now a NumPy Array):\n",
    "scaled_X = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's transform the scaled data with PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# We will reduce the dimensions to 2:\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Transform the data\n",
    "Xt = pca.fit_transform(scaled_X)\n",
    "\n",
    "# Check the output shape!\n",
    "print('New Shape:', Xt.shape)\n",
    "\n",
    "# We can save these components to a new DataFrame\n",
    "components = pd.DataFrame(Xt, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Add the targets to help with plotting\n",
    "components['Target'] = target\n",
    "components.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice way to visualise this is to use a simple scatter plot using seaborn `scatterplot()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert binary labels to text:\n",
    "components['Target'] = components.Target.apply(lambda x: 'Benign' if x == 0 else 'Malignant')\n",
    "\n",
    "# Create scatter plot:\n",
    "plt.figure(figsize=(10, 8)) # Make figure larger!\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='Target', data=components);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like our classes look quite easily seperable using the features we have! This is good news for our predictive modelling. We can be quite confident that a machine learning method will perform well. If we were to use features from PCA to train a model how do we decide how many components to use? We could simply look at the cumulative sum of the explained variance ratio! Don't worry if you do not understand the next piece of code, the plot is more interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "pca = PCA().fit(scaled_X)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see easily now that ~6 components contain ~90% of the entire variance! Maybe this would be a good place to start if using these components as input to a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Splits\n",
    "\n",
    "Our goal is to build a great model that can predict diagnoses accurately, but to be effective it must also **generalize** to new observations. Therfore, we need to split our data into two subsets:\n",
    "\n",
    "1. A larger portion of our data called the 'training' set\n",
    "2. A smaller portion of the dataset which we can use to test the built model\n",
    "\n",
    "It is important that we test our models on an 'unseen' subset so that we can ensure that our model has the ability to generalise and has not 'overfit'.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.ibm.com/cloud/architecture/images/practices/model-over-fitting.png\" alt=\"Overfitting\" width=\"60%\"/>\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "[image source](https://www.ibm.com/cloud/architecture/images/practices/model-over-fitting.png)\n",
    "\n",
    "Lets use an 80%/20% split here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the features and labels (NumPy arrays)\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "\n",
    "print('X train shape:', x_train.shape)\n",
    "print('X test shape:', x_test.shape)\n",
    "print('Y train shape:', y_train.shape)\n",
    "print('Y test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Models\n",
    "\n",
    "There is a huge amount of different machine learning algorithms that we could apply to this problem. Popular algorithms include:\n",
    "\n",
    "- [Nearest Neighbours](https://scikit-learn.org/stable/modules/neighbors.html)\n",
    "- [Logistic Regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)\n",
    "- [Desision Tree](https://scikit-learn.org/stable/modules/tree.html)\n",
    "- [Random Forest](https://scikit-learn.org/stable/modules/ensemble.html#random-forests)\n",
    "- [Support Vector Machine](https://scikit-learn.org/stable/modules/svm.html)\n",
    "- [Neural Network](https://scikit-learn.org/stable/modules/neural_networks_supervised.html)\n",
    "\n",
    "Detailed explanations of algorithms is beyond the scope of this notebook, but clicking through the links above should get you started if you are interesting in learning about machine learning in more detail.\n",
    "\n",
    "In this notebook we will use the logistic regression algorithm to model our problem. \n",
    "\n",
    "In practise data scientists will train mutiple different algorithms to find the best performing solution. Every algorithm makes different assumptions about the data from which it generalizes in order to make predictions thus, the perfomance of a model depends on how well these assumptions correlate with underlying patterns in the data.\n",
    "\n",
    "**Logistic Regression**\n",
    "\n",
    "Despite its name, logistic regression is actually used for classification problems and is conceptually very similar to [linear regression](https://en.wikipedia.org/wiki/Linear_regression). Unlike linear regression however, an S-shaped curve (sigmoid) is used to fit our data rather than a straight line. Logistic regression is based of the [maximum likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) concept, estimating the parameters of an assumed probability distribution function given observed data. According to this estimation the observed data should be most probable. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://static.javatpoint.com/tutorial/machine-learning/images/linear-regression-vs-logistic-regression.png\" alt=\"Overfitting\" width=\"50%\"/>\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "[image source](https://static.javatpoint.com/tutorial/machine-learning/images/linear-regression-vs-logistic-regression.png)\n",
    "\n",
    "Logistic regression can also be used when the probabilities between two classes is required. What do you think arethe benefits of being able to provide a probablity estimation for a paticular prediction?\n",
    "\n",
    "Based on our observations from PCA it looks like our data is linearly seperable and hence logistic regression should be a good fit! Lets try to build a model using  a [logistic regression classifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) from scikit-learn. Although scaling is not strictly necessary, it can help speed up convergence and make coefficients easier to interpret:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Setup our standard scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale our training features\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "\n",
    "# Initialize our model (default parameters)\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Fit our model on the scaled training data (features, target)\n",
    "lr.fit(x_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was simple and fast! The model however is fairly useless to us if we have not performed any evaluation. How do we know if it can generalize to unseen data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Model evaluatuion is perfomed on a held-out set of observations to check that a model has the ability to generalize to unseen data. Here we chose to put aside 20% of our data for evaluation purposes. Firstly we need to make predictions for the testing observations using our logistic regression model. Remember that we also need to scale our testing data in the same way as the training data. Instead of using `.fit_transform()` we will use `.transform()` to keep consitency across our datasets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale our tetsing features\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# Make our predictions (binary)\n",
    "y_pred = lr.predict(x_test_scaled)\n",
    "print('Predictions:', y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy\n",
    "\n",
    "Accuracy is probbaly the most simple metric we could use to evaluate our model. Accuracy is a measure of how well our model can determine the true labels of input observations:\n",
    "\n",
    "$Accuracy = \\frac{Number\\ of \\ correct\\ predictions}{Total\\ number\\ of\\ predictions}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate and round to three d.p.\n",
    "accuracy_score = round(lr.score(x_test_scaled, y_test), 3)\n",
    "\n",
    "print('Accuracy of logistic regression model on test data:', str(accuracy_score * 100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although accuracy is a common evaluation metric it should be treated with caution! Accuracy is only a good metric if the dataset contains balanced classes. Suppose that a dataset contained 90% of class A and 10% of class B a model could achieve an accuracy of 90% by only predicting class A, which is not a great evaluation of our models ability to classify. This can be paticularly dangerous when we are using a model for diagnostic purposes where we may fail to diagnose a disease due to a poor evaluation. In our case 'malignant' is the minority class and hence we should be cautious when using such metrics.\n",
    "\n",
    "#### Confusion Matrix\n",
    "\n",
    "In a classifcation problem there are four possible types of classification outcome:\n",
    "\n",
    "- True positives (success!)\n",
    "- False positives (failure :< type 1)\n",
    "- True negatives (sucess!)\n",
    "- False negatives (failure :< type 2)\n",
    "\n",
    "A confusion matrix is a nice method of visualising these outcomes in a grid form where we wish to see higher values on the diagonal. The confusion matrix also forms the basis for many other metrics:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://2.bp.blogspot.com/-EvSXDotTOwc/XMfeOGZ-CVI/AAAAAAAAEiE/oePFfvhfOQM11dgRn9FkPxlegCXbgOF4QCLcBGAs/s1600/confusionMatrxiUpdated.jpg\" alt=\"Overfitting\" width=\"50%\"/>\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "[image source](https://2.bp.blogspot.com/-EvSXDotTOwc/XMfeOGZ-CVI/AAAAAAAAEiE/oePFfvhfOQM11dgRn9FkPxlegCXbgOF4QCLcBGAs/s1600/confusionMatrxiUpdated.jpg)\n",
    "\n",
    "Let's plot a confusion matrix for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "# Construct a confusion matrix (NumPy array)\n",
    "c_m = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "ConfusionMatrixDisplay(c_m, display_labels=['Benign', 'Malignant']).plot(cmap='Blues');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What conclusions can you make from the confusion matrix? What errors does the model make? Is it better to diagnose malignant falsely or diagnose benign falsely?\n",
    "\n",
    "#### Receiver Operating Characteristic (ROC) Curve\n",
    "\n",
    "The ROC curve is a visual method for summarising all possible confusion matrices at all threshold values. It is a very popular method for evaluating a binary classifier. The ROC curve is a plot of the True Positive Rate (Recall/Sensitivity) vs the False Positive Rate. A diagonal line would indicate that a models performance is random (50% chance of outputting either label). The Area Under the Curve (AUC) of a classifier is equal to the probability that the classifier will rank a randomly chosen positive example higher than a randomly chosen negative example and thus is a usefull numerical metric for evaluating a classification model:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/max/1400/1*uC8BcLIMqYTmmojrFFzB9g.png\" alt=\"Overfitting\" width=\"40%\"/>\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "[image source](https://miro.medium.com/max/1400/1*uC8BcLIMqYTmmojrFFzB9g.png)\n",
    "\n",
    "Let's calculate the AUC and plot a ROC curve for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay, roc_curve, auc\n",
    "\n",
    "# First calculate our FPR and TPR\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "\n",
    "# Calculate the AUC from FPR and TPR\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot the ROC curve\n",
    "RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='Logistic Regression').plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What can you observe from the calculated metrics? \n",
    "- Do you think this is a good model?\n",
    "- Do you think the training data is large enough to construct a good model?\n",
    "- Do you think that the model is robust enough for real-life use?\n",
    "\n",
    "In reality a data scientist would use [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) techniques to get more reliable estimates for evaluation metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "We have now trained and evaluated a logistic regression model, but how can we interpret the model? What features is it using, or not using to make a prediction? Model interpretation is an active area of research and is essential if we want to use and trust machine learning models in real-life scenarios. Luckily for us we can use the models coefficients as a crude form of 'feature importance'. \n",
    "\n",
    "> The logistic regression coefficient β associated with a predictor X is the expected change in log odds of having the outcome per unit change in X. So  increasing the predictor by 1 unit (or going from 1 level to the next) multiplies the odds of having the outcome by eβ.\n",
    "\n",
    "Since our data is scaled all units are equivalent and hence our coefficients are comparable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = lr.coef_[0]\n",
    "\n",
    "feat_importance = pd.DataFrame({\n",
    "    'feature': features.columns,\n",
    "    'coef': coefficients\n",
    "})\n",
    "\n",
    "feat_importance.sort_values('coef', inplace=True)\n",
    "ax = feat_importance['coef'].plot.barh(figsize=(12, 10))\n",
    "ax.set_title('Logistic Regression Coefficients')\n",
    "ax.set_yticklabels(feat_importance.feature)\n",
    "ax.set_xlabel('Correlation');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do these 'feature importances' compare with the pearsons r scores we calculated earlier?\n",
    "\n",
    "Of course there are more complicated ways of calculating feature importances which are more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Predictive modelling is a huge part of the data-science landscape today.\n",
    "\n",
    "- Predictive modelling is the application of statistical models to predict future outcomes\n",
    "- Typically predictive statistical models involve machine learning\n",
    "- Data exploration is essential to building a successful model\n",
    "- Correct model evaluation is paticularly important for real-world application\n",
    "\n",
    "Feel free to add more code cells and experiment with the concepts you have learnt.\n",
    "\n",
    "If you want to know more there are some extra resources from external sources linked in the beginning section. You can click the link below to go back to the top.\n",
    "\n",
    "Click [here](#Contents) to go back to the contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
