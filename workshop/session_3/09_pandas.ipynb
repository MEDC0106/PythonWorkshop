{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ff23c01-e703-444a-a3d6-ebefbdfa8d9c",
   "metadata": {},
   "source": [
    "### MEDC0106: Bioinformatics in Applied Biomedical Science\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../../resources/static/Banner.png\" alt=\"MEDC0106 Banner\" width=\"90%\"/>\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "---------------------------------------------------------------\n",
    "\n",
    "# 09 - Pandas\n",
    "\n",
    "*Written by:* Oliver Scott\n",
    "\n",
    "**This notebook provides a general introduction to Pandas.**\n",
    "\n",
    "Feel free to modify the code cells to see how things work!\n",
    "\n",
    "### What is Pandas?\n",
    "\n",
    "**[Pandas](https://pandas.pydata.org/)** is a powerful Python package for data analysis, offering functions for analysing, cleaning, and manipulating data. It is one of the most crucial tools for data scientists and forms the backbone of many Python-based data science projects.\n",
    "\n",
    "Data manipulation with Pandas is often the first step before further analysis using other Python packages, such as:\n",
    "- **[SciPy](https://www.scipy.org/)** for statistical analysis,\n",
    "- **[Matplotlib](https://matplotlib.org/)** for data visualization,\n",
    "- **[scikit-learn](https://scikit-learn.org/stable/)** for machine learning.\n",
    "\n",
    "These tools are part of the Python scientific stack, essential for informatics or data science careers. To be effective in Pandas, it is helpful to have a solid understanding of core Python concepts (as covered in the first session) and some familiarity with NumPy (covered in the supplementary notebook for Session 3). If any concepts are challenging, reviewing previous notebooks might be helpful.\n",
    "\n",
    "This notebook covers the basics of Pandas. While Pandas is a comprehensive library that could fill an entire course, here weâ€™ll focus on fundamental concepts to get you started and provide a foundation for further learning.\n",
    "\n",
    "-----\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [The basics](#The-basics)\n",
    "2. [Creating DataFrames](#Creating-DataFrames)\n",
    "3. [Reading data](#Reading-data)\n",
    "4. [Essential operations](#Essential-operations)\n",
    "5. [Slicing and selecting](#Slicing-and-selecting)\n",
    "5. [Arithmetic operations](#Arithmetic-operations)\n",
    "6. [Applying functions](#Applying-functions)\n",
    "7. [Time-series](#Time-series)\n",
    "8. [Plotting](#Plotting)\n",
    "\n",
    "-----\n",
    "\n",
    "### Extra resources:\n",
    "\n",
    "- [Pandas Getting Started Guide](https://pandas.pydata.org/pandas-docs/stable/getting_started/index.html) - Introduction to Pandas\n",
    "- [RealPython-01](https://realpython.com/pandas-python-explore-dataset/) - Introduction to Pandas\n",
    "- [RealPython-02](https://realpython.com/pandas-dataframe/) - Pandas DataFrames\n",
    "\n",
    "-----\n",
    "\n",
    "### References:\n",
    "\n",
    "- [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "- [Learn Data Science](https://www.learndatasci.com/tutorials/python-pandas-tutorial-complete-introduction-for-beginners/)\n",
    "\n",
    "-----\n",
    "\n",
    "## The basics\n",
    "\n",
    "Importing `pandas` is no different to any other package/module. Pandas users often use the `pd` alias to keep code clean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff02cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a Pandas Series\n",
    "s = pd.Series([1.0, 2.0, 3.0, 5.0])\n",
    "\n",
    "# Display the Series\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca29791b",
   "metadata": {},
   "source": [
    "### Core Components\n",
    "\n",
    "Pandas has two core components, the `Series` and the `DataFrame`.\n",
    "\n",
    "The `Series` can be imagined as a single column in a data table, whereas the `DataFrame` can be imagined as a full data table made up of multiple `Series`. Both types have a similar interface allowing the user to perform similar operations. DataFrames are similar to spreadsheets that you may have interacted with in software such as Excel. DataFrames are often faster, easier to use and more powerful than spreadsheets.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.datasciencemadesimple.com/wp-content/uploads/2020/05/create-series-in-python-pandas-0.png?ezimgfmt=rs%3Adevice%2Frscb1-1\" alt=\"Pandas DataFrame\" width=\"70%\"/>\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "[Image source](https://www.datasciencemadesimple.com/wp-content/uploads/2020/05/create-series-in-python-pandas-0.png?ezimgfmt=rs%3Adevice%2Frscb1-1)\n",
    "\n",
    "-----\n",
    "\n",
    "## Creating DataFrames\n",
    "\n",
    "There are numerous ways to create a DataFrame in Pandas. In most cases it is likely that you will want to read in data from a particular file, however DataFrames can also be constructed from scratch from lists, tuples, NumPy arrays or Pandas Series. One of the simplest ways to create a DataFrame is from a Python dictionary, where keys become column names and values become the column data. \n",
    "\n",
    "Suppose we wanted to construct a table like the one below:\n",
    "\n",
    "| PatientID | Gender | Age | Outcome  |\n",
    "|-----------|--------|-----|----------|\n",
    "| 556785    | M      | 19  | Negative |\n",
    "| 998764    | F      | 38  | Positive |\n",
    "| 477822    | M      | 54  | Positive |\n",
    "| 678329    | M      | 22  | Negative |\n",
    "| 675859    | F      | 41  | Negative |\n",
    "\n",
    "For this we can use the default constructor `pd.DataFrame()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f056dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our dictionary containing the raw data\n",
    "data = {\n",
    "    'PatientID': [556785, 998764, 477822, 678329, 675859],\n",
    "    'Gender': ['M', 'F', 'M', 'M', 'F'],\n",
    "    'Age': [19, 38, 54, 22, 41],\n",
    "    'Outcome': ['Negative', 'Positive', 'Positive', 'Negative', 'Negative']\n",
    "}\n",
    "\n",
    "# We can now construct a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df  # Show the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc13b8b-e017-410b-bb23-17314352dc8c",
   "metadata": {},
   "source": [
    "Notice how there is also an unnamed column containing the numbers 0-4. This is the **index** of each row. In fact, you may also specify a custom index when constructing a dataframe; (`pd.DataFrame(data, index=['Tom', 'Joanne', 'Joe', 'Xander', 'Selena'])`). In this case the index is the name of the patient.\n",
    "\n",
    "When working with large tables of data, viewing the entire table is often impractical. Pandas provides the `.head()` method to display the first few rows and `.tail()` to display the last few rows. These methods are useful for quickly inspecting the top or bottom of a DataFrame without loading the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fadeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first three rows\n",
    "df.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22317633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the last two rows\n",
    "df.tail(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36954f2",
   "metadata": {},
   "source": [
    "Accessing an individual column in a DataFrame is straightforward and uses the same syntax as accessing a key in a Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb971546",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_column = df['Gender']\n",
    "gender_column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb4a103",
   "metadata": {},
   "source": [
    "If the column label is a string, you can also use **dot-syntax** to access the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56909d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_column = df.Age\n",
    "age_column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e1b797",
   "metadata": {},
   "source": [
    "## Reading data\n",
    "\n",
    "Reading and writing data in various file formats is an essential part of the data analysis pipeline. Pandas can handle data from multiple file types, including CSV, JSON, Excel, SQL, and [many more](https://pandas.pydata.org/pandas-docs/stable/reference/io.html).\n",
    "\n",
    "In the folder `data` we have provided a dataset downloaded from the [UK government](https://coronavirus.data.gov.uk/details/cases?areaType=overview&areaName=United%20Kingdom) detailing the number of reported positive COVID-19 test results in the United Kingdom by date reported (up to Oct-31-21). The file is in the CSV format and can be read using Pandas with the method `.read_csv()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02403a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data_path = 'https://raw.githubusercontent.com/MEDC0106/PythonWorkshop/main/workshop/session_3/data/data_2021-Oct-31.csv'  # This is the path to our data\n",
    "\n",
    "cv_data = pd.read_csv(cv_data_path)\n",
    "cv_data.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2454d1",
   "metadata": {},
   "source": [
    "You can also easily save a DataFrame to a new CSV file using the `df.to_csv()` method.\n",
    "\n",
    "```python\n",
    "cv_data.to_csv('coronavirus_testing_results.csv')\n",
    "```\n",
    "\n",
    "Give it a try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3033e0-b445-4ec6-b495-72eeaedcf938",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4d63150",
   "metadata": {},
   "source": [
    "## Essential operations\n",
    "\n",
    "Now that weâ€™ve loaded data into a DataFrame, we can start analyzing it. Typically, after loading data, itâ€™s helpful to view it briefly to ensure it looks correct and to understand the values youâ€™re working with. Weâ€™ve already discussed visualising data with `.head()` and `.tail()`. Another useful method is `.info()`, which provides essential details about your dataset, such as the number of rows and columns, non-null counts, data types for each column, and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0361030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43083d5",
   "metadata": {},
   "source": [
    "Notice that we have 6 columns, with four of type `object` (often representing strings) and two of type `int64` (integers). The `.info()` output also shows that we have 2466 non-null values, meaning no missing values in this dataset. Knowing the data type of each column is important because it determines which operations we can perform on them (e.g., we would not calculate the mean of a column containing strings). \n",
    "\n",
    "You can also use `.shape` to see the number of rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c560389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a556e47",
   "metadata": {},
   "source": [
    "#### Removing duplicates\n",
    "\n",
    "Input data is often noisy and requires cleaning before analysis. One common issue is duplicated rows, which can skew statistical analysis. Fortunately, Pandas provides tools to handle this easily. Since our current data does not contain duplicates, we will create some by duplicating the existing data and appending it to itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3a0eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated = pd.concat([cv_data, cv_data])  # Here we have copied the data and added it to itself\n",
    "duplicated.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c096274",
   "metadata": {},
   "source": [
    "Notice that we assign the result of the `.concat()` method to a new variable, which creates a copy of the data without altering the original DataFrame. We can now easily remove duplicates using the `.drop_duplicates()` method. Reviewing the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop_duplicates.html) is helpful to explore additional arguments and options provided by this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f043ddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated = duplicated.drop_duplicates()\n",
    "duplicated.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396eb7f7",
   "metadata": {},
   "source": [
    "Notice that the shape of the data is now the same as the original, confirming that duplicates have been removed.\n",
    "\n",
    "Also, observe that, again, we assigned the result to a new variable with the same name. This pattern can become cumbersome, so Pandas offers an `inplace` argument. By setting `inplace=True`, Pandas performs the operation directly on the original DataFrame, eliminating the need to assign it to a new variable.\n",
    "\n",
    "```python\n",
    "duplicated.drop_duplicates(inplace=True)  # no need to assign to a new variable\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8b5fa4",
   "metadata": {},
   "source": [
    "#### Removing null values\n",
    "\n",
    "Raw data often contains missing values that need to be addressed before analysis. Missing values are typically represented by `None` or `np.nan`. There are two main options for handling missing data:\n",
    "\n",
    "1. Remove all rows with missing values.\n",
    "2. Impute (fill in) the missing values.\n",
    "\n",
    "Here, we will focus on the first approach.\n",
    "\n",
    "Since our data is already clean and contains no null values, weâ€™ll add a new column with some null values for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4b7ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "n_rows = len(cv_data)  # Get the number of rows in the `cv_data` DataFrame\n",
    "\n",
    "# Create a list with 20% None values and 80% ones\n",
    "nulls_or_ones = []\n",
    "for _ in range(n_rows):\n",
    "    if random.random() < 0.2:  # 20% chance of None\n",
    "        nulls_or_ones.append(None)\n",
    "    else:\n",
    "        nulls_or_ones.append(1)\n",
    "\n",
    "print(\"Prepared a random list of length:\", len(nulls_or_ones))\n",
    "print(\"Is the first value null?\", pd.isna(nulls_or_ones[0]))  # Check if the first value is None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019accbc",
   "metadata": {},
   "source": [
    "Now add a column to the `cv_data` DataFrame containing our newly constructed list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4234966f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data['RandomData'] = nulls_or_ones  # Make a colum called 'RandomData'\n",
    "cv_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5371d82",
   "metadata": {},
   "source": [
    "We can also see now that we have null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea17e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa17ec2",
   "metadata": {},
   "source": [
    "The `.isnull()` or `.isna()` methods return a DataFrame of the same shape as the original, with `True` for `NaN` or `None` values and `False` for non-null values. They allow you to identify which entries are missing.\n",
    "\n",
    "Applying `.sum()` after `.isnull()` or `.isna()` counts `True` values, i.e. nulls, in each column, giving a count of missing values per column. This works because `True` is treated as 1 in summation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200c2964",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d52bb5c",
   "metadata": {},
   "source": [
    "In data analysis, you often need to decide whether to remove or impute missing values. Removing data is generally advisable only if the number of missing values is small. To remove rows with null values, you can use the `.dropna()` [method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html). This method removes any row containing at least one null value and returns a new DataFrame unless `inplace=True` is specified.\n",
    "\n",
    "If you want to drop columns with null values instead, you can change the axis of operation by setting `axis=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63f6b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's remove rows with null values\n",
    "remove_rows = cv_data.dropna()\n",
    "remove_rows.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b618363",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_rows.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17042a9a",
   "metadata": {},
   "source": [
    "Now let's change the axis and remove the `RandomData` colum we injected into the `cv_data` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bca7ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.dropna(axis=1, inplace=True)  # We can do it inplace since we do not care about this column\n",
    "cv_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8102610-b564-4f74-a9eb-98a4706971e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd796fbe",
   "metadata": {},
   "source": [
    "#### Understanding data\n",
    "\n",
    "Now that your data is cleaner, itâ€™s time to perform some basic statistical analysis to understand each column. This initial exploration helps inform the next steps in the analysis and may guide how to visualise the data. Pandas provides an easy way to get a quick summary of the distribution of continuous variables using `.describe()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bc08ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d134c69",
   "metadata": {},
   "source": [
    "We can also do the same for categorical columns, but we will have to do it seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9948f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.areaName.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8576647",
   "metadata": {},
   "source": [
    "This shows us that in this dataset there are four unique area names with `'England'` being the most frequent. We can also check the unique values using the `.unique()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cd9c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.areaName.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15723e8",
   "metadata": {},
   "source": [
    "We can see that the dataset contains data for:\n",
    "\n",
    "- England,\n",
    "- Northern Ireland,\n",
    "- Scotland,\n",
    "- and Wales.\n",
    "\n",
    "But how many times are each of these values recorded? We can use the method `.value_counts()` to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a81e60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.areaName.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b944b2",
   "metadata": {},
   "source": [
    "## Slicing and selecting\n",
    "\n",
    "In the previous section we saw how to produce summaries of entire data which is useful. However, sometimes we will want to perform analyses on certain subsets of data. We have already seen how to extract a column of data using square brackets or dot-syntax, i.e. `df['col']` or `df.col`, and now we will dive deeper into the Pandas selection language. \n",
    "\n",
    "When selecting parts of a DataFrame we may be returning either a `DataFrame` or a `Series`. It is important to know which is returned so that you use the correct syntax.\n",
    "\n",
    "#### Selecting by column(s)\n",
    "\n",
    "Using the square-bracket syntax will return a Pandas `Series`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93126ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cv_data['areaName'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258bffb9",
   "metadata": {},
   "source": [
    "If you wish to access it as a DataFrame you can supply the column name as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fbaeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cv_data[['areaName']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafab336",
   "metadata": {},
   "source": [
    "Adding another column to our selection is as simple as adding another column name to the list. Obviosuly in this case our code will return a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9829f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = cv_data[['areaName', 'areaCode']]\n",
    "type(selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b416a062",
   "metadata": {},
   "source": [
    "#### Selecting by row(s)\n",
    "\n",
    "Selecting rows in a DataFrame can be done using two methods:\n",
    "\n",
    "- `.loc`: Locates rows by label or name.\n",
    "- `.iloc`: Locates rows by numerical index.\n",
    "\n",
    "Since our data has a numerical index, it makes sense to use `.iloc` for row selection. If the data had an index with string labels, `.loc` would be the appropriate choice for selecting rows by name. However, `.iloc` would still work, returning the row at the specified numerical position rather than by name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02fee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.loc[222]  # Return the row with index 222"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3829394e",
   "metadata": {},
   "source": [
    "We can also use slices to select a range of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5780f016",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.loc[222:226]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb23b27",
   "metadata": {},
   "source": [
    "#### Conditional selections\n",
    "\n",
    "Selecting data by index is helpful, but it can be limiting if we donâ€™t know what the indexes correspond to. For example, if we only want the data from Wales, we can use conditional selections to filter the DataFrame based on specific criteria.\n",
    "\n",
    "Pandas DataFrames can be filtered by using a Boolean array, Series, or DataFrame generated from a conditional expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb237f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = cv_data['areaName'] == 'Wales'  # Creates a Boolean Series\n",
    "ind.tail(5)  # Displays the last 5 values of the Boolean Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46634e40",
   "metadata": {},
   "source": [
    "Using this Boolean Series we can index the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa99747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wales_data = cv_data[ind]\n",
    "wales_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750fe48b",
   "metadata": {},
   "source": [
    "We can simplify this quite nicely into a one line expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf6c365",
   "metadata": {},
   "outputs": [],
   "source": [
    "wales_data = cv_data[cv_data['areaName'] == 'Wales']\n",
    "wales_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc49481",
   "metadata": {},
   "source": [
    "We can apply this to numerical columns also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caebede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows where number of reported positives is less than 100\n",
    "cv_data[cv_data['newCasesByPublishDate'] < 100].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fba9463",
   "metadata": {},
   "source": [
    "Chaining conditional expressions allows us to create powerful selections. For this we can use the logical operators `|` and `&`. \n",
    "\n",
    "**Remember to put separate conditions in parentheses!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535e434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count dates in England where numer of reported positives is more than 10,000\n",
    "cv_data[(cv_data['areaName'] == 'England') & (cv_data['newCasesByPublishDate'] > 10000)].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2484e3",
   "metadata": {},
   "source": [
    "## Arithmetic operations\n",
    "\n",
    "Basic arithmentic operations can be applied simultaneously on all rows in a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6014c80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.newCasesByPublishDate / 100  # Divides a column by 100 and returns a Pandas Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbce6ed",
   "metadata": {},
   "source": [
    "You may also perform arithmetic operations between columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72d8ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.newCasesByPublishDate + cv_data.cumCasesByPublishDate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d84151",
   "metadata": {},
   "source": [
    "You can inject a new column (we will call it `Rubbish`) with the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e69e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data['Rubbish'] = cv_data.newCasesByPublishDate * 0.3 / cv_data.cumCasesByPublishDate\n",
    "cv_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4152af",
   "metadata": {},
   "source": [
    "Pandas also provides some other handy functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8d7f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_data.newCasesByPublishDate.mean())\n",
    "print(cv_data.newCasesByPublishDate.std())  # Standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9611e97e",
   "metadata": {},
   "source": [
    "## Applying functions\n",
    "\n",
    "While it is possible to iterate over a Pandas DataFrame or Series, it is slow in Python. \n",
    "\n",
    "We can use the `.apply()` method to apply a function to each element in a column or across columns. We can also save this result to a new column. \n",
    "\n",
    "Let's create an arbitrary function that we can apply to the data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bde53e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorise_cases(x):\n",
    "    if x >= 10000:\n",
    "        return 'High'\n",
    "    elif x <= 200:\n",
    "        return 'Low'\n",
    "    else:\n",
    "        return 'Medium'\n",
    "\n",
    "# No output is expected from this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8697028",
   "metadata": {},
   "source": [
    "The above function categorises a case count into arbritarty categories: 'High', 'Medium' and 'Low'. Now we can apply this to the `newCasesByPublishDate` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac5c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data['Category'] = cv_data['newCasesByPublishDate'].apply(categorise_cases)\n",
    "cv_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66be8c5d",
   "metadata": {},
   "source": [
    "Users often will use anonymous functions instead of defining an explicit function like above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa78cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data['newCategory'] = cv_data['newCasesByPublishDate'].apply(lambda x: 'Red' if x >= 20000 else 'Amber')\n",
    "cv_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbc86a6",
   "metadata": {},
   "source": [
    "## Time-series\n",
    "\n",
    "One of the columns contains dates as strings (`object` type), which isnâ€™t very useful for time-based analysis. Pandas has a `datetime` type that allows for more advanced selections and operations based on time spans. First, we need to convert our column to a `datetime` format using the `.to_datetime()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0504066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data['date'] = pd.to_datetime(cv_data['date'])\n",
    "cv_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17844cc8",
   "metadata": {},
   "source": [
    "Now we have the `date` column values in this form we can make selections within time ranges using the `.between()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caa3f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's select data between the 20th and the 30th October 2021 and restrict it to England\n",
    "selection = cv_data[(cv_data.date.between('2021-10-20','2021-10-30')) & (cv_data.areaName == 'England')]\n",
    "selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935b7200",
   "metadata": {},
   "source": [
    "Working with time-series data is even more powerful if we use them as our index. Let's first only consider `'Scotland'` in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4129bb0b-359e-4c90-aefe-0fb902ea6f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scotland_data = cv_data[cv_data.areaName == 'Scotland']\n",
    "scotland_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2357c9fe",
   "metadata": {},
   "source": [
    "Now we can set the `date` column of the `scotland_data` DataFrame as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9714dec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scotland_data.set_index('date', inplace=True)\n",
    "scotland_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59704f8b-bdf8-40fd-9a66-f64b99c7ac77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "466e23fa",
   "metadata": {},
   "source": [
    "You may have noticed that the data is in time-descending order. We may want to reverse this ordering. Now that the index is the `date` column we can do this easily using the `.sort_index()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0470a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scotland_data.sort_index(inplace=True)\n",
    "scotland_data  # Show the data in time-ascending order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b5ddb1",
   "metadata": {},
   "source": [
    "We can use slicing with `.loc` to select a date range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8579569",
   "metadata": {},
   "outputs": [],
   "source": [
    "scotland_data.loc['2021-10-20':'2021-10-30']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ba0b88",
   "metadata": {},
   "source": [
    "We can **resample** time-series data into different intervals and calculate statistics for each interval. \n",
    "\n",
    "For example, the code below resamples the data into 10-day intervals and calculates the mean of `newCasesByPublishDate` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17e9bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled = scotland_data.resample(rule='10d')['newCasesByPublishDate'].mean()\n",
    "resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c88aca",
   "metadata": {},
   "source": [
    "Instead of `.mean()`, you could use other aggregation functions such as `.min()`, `.max()`, or `.sum()`. Additionally, you can calculate a rolling statistic using `.rolling()` and a specified window size. For instance, below we calculate a rolling average with a 10-day window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d5be13",
   "metadata": {},
   "outputs": [],
   "source": [
    "scotland_data['rollingAvgTenDay'] = scotland_data.rolling(10)['newCasesByPublishDate'].mean()\n",
    "scotland_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e73e0f",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "Pandas enables easy visualization of data in DataFrames and Series by interfacing directly with the plotting package [Matplotlib](https://matplotlib.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3eda90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also add this 'Jupyter magic' to display plots in the notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# No output is expected from this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2601bf3",
   "metadata": {},
   "source": [
    "Creating a plot from Pandas DataFrames is as simple as calling `.plot()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be89002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scotland_data.newCasesByPublishDate.plot();  # Adding a semicolon is preferred in Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e4dfff",
   "metadata": {},
   "source": [
    "We could have achieved the same result by using the following syntax too:\n",
    "\n",
    "```python\n",
    "scotland_data.newCasesByPublishDate.plot.line();\n",
    "```\n",
    "\n",
    "These plotting functions offer various [arguments](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) to adjust the appearance of your plots. These arguments are passed to the underlying Matplotlib methods, allowing for fine-tuning. You can also specify different types of plots; for instance, you could visualise the data as a box plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce051d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a time window (1-month)\n",
    "window = scotland_data['2021-09-30':'2021-10-30']\n",
    "\n",
    "window.newCasesByPublishDate.plot.box();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49af2b3",
   "metadata": {},
   "source": [
    "Do you notice anything unusual?\n",
    "\n",
    "<details>\n",
    "<summary>Click here for hint</summary>\n",
    "In the box plot, we see an outlier above the upper whisker, represented by a single point around 3500. This indicates that there is a data point with a significantly higher value than the rest of the distribution for <em>newCasesByPublishDate</em>. Outliers like this can impact analysis, so it's often a good idea to investigate these values further to understand their context or consider handling them differently, depending on your analysis goals.\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's plot the raw data along with the 10-day rolling average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570b49eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scotland_data.newCasesByPublishDate.plot(figsize=(8, 4));  # Also specify the size\n",
    "scotland_data.rollingAvgTenDay.plot();\n",
    "plt.legend();  # We can also add a legend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59482120",
   "metadata": {},
   "source": [
    "We can also save figures using `.savefig()`. Check the `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24c7b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = scotland_data.newCasesByPublishDate.plot(figsize=(8, 4)).get_figure()\n",
    "figure.savefig('Scotland.png');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef67f9d",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Cleaning, analysing, manipulating and visualising data is an essential skill for a data scientist. In fact, 80% of a data scientist's job is cleaning data for analysis.\n",
    "\n",
    "Feel free to add more code cells and experiment with the concepts you have learnt.\n",
    "\n",
    "You can use this notebook as reference if you need to refresh your knowledge on any of the concepts explored.\n",
    "\n",
    "If you want to learn more there are some extra external resources linked at the beginning of this notebook. You can click [here](#Contents) to go back to the top."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
